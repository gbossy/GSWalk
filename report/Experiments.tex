\documentclass[12pt]{article}
\usepackage[margin=68pt]{geometry}%48 originally, commented originally
%
\usepackage{amsthm}
\usepackage{amsmath}
%\usepackage{parskip}
\usepackage{rotating}
\usepackage{cite}
\usepackage{lmodern} % fix problems with typewriter font
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage%[demo]
{graphicx}
\usepackage{subcaption}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{pgf,tikz,pgfplots}

%to include pdf plots
\usepackage{pdfpages}

%To do loops
\usepackage{multido}

%to include plots generated in python
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\DeclareMathOperator{\Span}{span}

\DeclareMathAlphabet{\mymathbb}{U}{BOONDOX-ds}{m}{n}
\usepackage[linesnumbered,boxruled]{algorithm2e}%ruled in 1st param
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage%[compact]
{titlesec}
  %  \titlespacing{\section}{0pt}{2ex}{1.ex}
  %  \titlespacing{\subsection}{0pt}{1.5ex}{1ex}
   % \titlespacing{\subsubsection}{0pt}{1ex}{1ex}
%\usepackage[usenames]{color}
    \setlength{\parskip}{0.23cm}
    \setlength{\parindent}{1em}
    
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\dom}{$dom$}
\title{Studying the Gram-Schmidt Walk}
\author{Gaëtan Bossy, under the supervision of Adam Marcus\\Chair of Combinatorial Analysis}
\begin{document}
\maketitle
%Issue with lindisc(point rather than [0,1]^n vector
\begin{center}\bf Abstract\end{center}
\small TODO

%say that vectors are sampled uniformly everytime we talk about a ball

\section{Introduction}
In 1997, polish mathematician Wojciech Banaszczyk published \textit{Balancing vectors and Gaussian measures of $n$-dimensional convex bodies}\cite{banaszczyk1998balancing} in which we can read the following theorem:
\begin{theorem}[Banaszczyk's Theorem]\label{banaszczyk}
For all convex body $K \subseteq \mathbb{R}^m$, with Gaussian measure $\gamma_m(K)\geq 1/2$, and given $\textbf{v}_1, \dots, \textbf{v}_n \in \mathbb{R}^m$, $\|\textbf{v}_i\|_2 \leq 1$ for all $i$, then there exists $ \textbf{x} \in \{-1, 1\}^n$ such that
$$\sum_{i=1}^n \textbf{x}(i)\textbf{v}_i \in 5 \cdot K $$
\end{theorem}
The Gaussian measure of a body is defined as $$\gamma_m(S) = \mathbb{P}[g \in S] = \int_{y \in S} \frac{1}{(2 \pi)^{m/2}} e^{-||y||^2/2} dy$$
    where $g$ is a standard Gaussian random vector in $\mathbb{R}^m$, i.e. $g \sim \mathcal{N}(0, I_m)$. 

The proof of this theorem was non-constructive though, but nearly 20 years later, Dadush, Garg, Lovett and Nikolov showed that in order to get a constructive algorithm, all that is needed is a subgaussian distribution sampler. 

\begin{theorem}[\cite{construct}]\label{equivalence}
Banaszczyk's Theorem (up to the constant) is equivalent to the following statement:\\
Let $\textbf{v}_1, \dots, \textbf{v}_n \in \mathbb{R}^m$ of $\ell_2$ norm at most 1, with $\textbf{B}= (\textbf{v}_1, \dots, \textbf{v}_n) \in \mathbb{R}^{m \times n}$ and $K \subseteq \mathbb{R}^m$ a convex body  of Gaussian measure at least 1/2.
Then, there exists $\textbf{x}_0 \in [-1,1]^n$ and there exists a distribution $D$ on $\{-1,1\}^n$, such that: 
\begin{enumerate}
    \item  $\textbf{B}\textbf{x}_0 = \sum_{i=1}^n \textbf{x}_0(i)\textbf{v}_i \in K$\\
    \item  for $\textbf{x} \sim D$, $\textbf{B}(\textbf{x}-\textbf{x}_0)= \sum_{i=1}^n (\textbf{x}(i)-\textbf{x}_0(i))\textbf{v}_i$ is $\sigma$-subgaussian, for some $\sigma >0$ and if $\textbf{x}_0(i) \in \{-1, 1\}$, $\mathbb{P}[\textbf{x}(i)=\textbf{x}_0(i)]=1, \forall i$.
    \item $\mathbb{P}_{\textbf{x} \sim D}[\sum_{i=1}^n (\textbf{x}(i)- \textbf{x}_0(i))\textbf{v}_i \in c(\sigma)K] \geq 1/2$ for some $c(\sigma)$.
\end{enumerate}
\end{theorem}
In this last theorem, one can just pick $\textbf{x}_0=\textbf{0}$ because the gaussian measure bound forces it to be inside $K$, but finding the subgaussian distribution is far from trivial. The proof uses the Minimax theorem from Von Neumann \cite{neumann1928theorie}, turning the choice of the distribution in a game, and a result from Talagrand \cite{talagrand2005generic} on the norm of subgaussian vectors. In addition to all that, there is a complicated recentering procedure to show that the result still holds for non-symmetric bodies.

A couple years later, a team including three of the previous authors, Dadush, Garg and Lovett, joined by Bansal this time, actually found and analysed an algorithm to succesfully sample colorings that allows us to construct the colorings described by Banaszczyk in his work from my birth year. They call this algorithm the Gram-Schmidt walk, because it uses a procedure that can remind of Gram-Schmidt Orthogonalization. 

The Gram- Schmidt walk was first introduced in \cite{blues}. It was described as a way to get a constructive algorithm for Theorem \ref{ banaszczyk}, but was later also used for experiment design by Spielman, Harshaw, Zhang and Sävje in \cite{harshaw2019balancing}. They use the balancing power of the algorithm to explicitly navigate the tradeoff between experiment unit covariate balance and robustness of the design. They improved the constant in the subgaussianity result and proved several other results which let them document the expected behavior of their design.

In this work, we will try to understand the algorithm, think about how to modify it to achieve different goals, and what other things we could use it for.

\section{The Algorithm}
%Define v_perp A_i etc
The algorithm will take as input $\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$, and an initial coloring $\textbf{z}_0\in[-1,1]^n$. It will consist of $n$ time steps. At the end of time step $t$, we obtain a fractional coloring $\textbf{z}_t\in[-1,1]^n$. An element $i \in [n]$ is said to be \textit{alive} at time $t$ if $|\textbf{z}_{t-1}(i)|<1$, and \textit{fixed} otherwise. Let $A_t=\{i\in[n]:|\textbf{z}_{t-1}(i)|<1\}$. The \textit{pivot} $n(t)$ is an element that is alive at time $t$, which can for example be chosen randomly or as the largest indexed element, among the elements that are still alive. We define the set $V_t$ as $\Span\{\textbf{v}_i:i\in A_t,i\not=n(t)\}$. We denote by $\Pi_{V_t^\perp}$ the projection operator on $V_t^\perp$. Finally, we will need $\textbf{v}^{\perp}(t)=\Pi_{V_t^\perp}(\textbf{v}_{n(t)})$ as the projection of the pivot vector over all alive vectors. We are now ready to discover the actual pseudocode of the algorithm.

It produces an assignment $z$ such that the corresponding vector of imbalances, also referred to as the output sum of groups, has a small norm. Another way to see it is that it divides the vectors in two groups such that the sum of the vectors in each group are pretty close.

\begin{algorithm}[H]
{\fontsize{10}{12}
\caption{The Gram-Schmidt Walk by \cite{blues}}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$%with $\ell_2$ norm  at most 1
, an initial coloring $\textbf{z}_0\in[-1,1]^n$}
    \Output{a coloring $\textbf{z}_n \in \{-1,1\}^n$}
   $A_1=\{i\in[n]:|\textbf{z}_0(i)|<1\}$, $n(1) = \max \{i \in A_1\}$ and $t=1$.\\
    \While{$A_t\not=\emptyset$}{
        Compute $\textbf{u}_t\in\mathbb{R}^n$ such that
        $\begin{cases}
            \textbf{u}_t(n(t)) =1\\
            \textbf{u}_t(i) =0 \text{ if } i \notin A_t\\
            \textbf{v}^\perp(t) = \textbf{v}_{n(t)} + \sum_{i \in A_t\setminus\{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
        \end{cases}$\\
        $\Delta = \{\delta : \textbf{z}_{t-1} + \delta \textbf{u}_t \in [-1,1]^n\}$, let $\begin{cases}
            \delta_t^+ = \max \Delta\\
            \delta_t^- = \min \Delta
        \end{cases}$
         then $\delta_t = \begin{cases}
            \delta_t^+ \text{ w.p. } \frac{-\delta_t^-}{(\delta_t^+ - \delta_t^-)}\\
            \delta_t^- \text{ w.p. } \frac{\delta_t^+}{(\delta_t^+ - \delta_t^-)}
        \end{cases}$\\
        $\textbf{z}_t = \textbf{z}_{t-1} + \delta_t \textbf{u}_t$, $t\leftarrow t+1$,  $A_t=\{i\in[n]:|\textbf{z}_{t-1}(i)|<1\}$, $n(t) = \max \{i \in A_t\}$.
    }
    Output $\textbf{z}_t\in\{-1,1\}^n$.
    %\caption{Gram-Schmidt walk}
    \label{walk}
    }%
    \end{algorithm}
Throughout the text, we will refer to this algorithm as the GSW. There is also a variant which always choose the delta with the smallest absolute value instead of being a martingale, which we will denote as the Deterministic Gram-Schmidt Walk, or DGSW. 

On can describe the algorithm as a walk: We start at a certain coloring, and at each time step we choose a direction and a length to move, then move, and repeat until we reach a vertex of the hypercube.

%The interesting part is how to choose the direction and length of the move in a smart way to stay as balanced as possible. Defining $v_\perp$ as the least we will move while pushing towards coloring the pivot, it then follows that we want to push as much as possible in that direction until an element gets colored, and our $v_\perp$ thus changes.

\subsection{Observations}
According to the choice of $\delta_t$, it is clear that at least one element gets frozen at each time step as $\delta_t$ is maximal such that $z_t+\delta_tu_t\in[-1,1]^n$. Thus the GSW algorithm runs in at most $n$ iterations. If we chose $\delta_t$ according to some more complicated distribution, it wouldn't be possible guarantee a coloration per time step and the algorithm would lose most of its appeal. Additionally, the update direction depends on the elements that are alive. Thus if they do not change between two time steps, the update direction would not change and we would just keep moving along the same line until we hit a border, which is why choosing one of the two maximal valid lengths, $\delta_+$ or $\delta_-$, is the best method to choose the length of the move.

We can see that
$$\mathbb{E}[\delta_t \mid \delta_t^-, \delta_t^+] = \delta_t^+ \cdot \frac{-\delta_t^-}{\delta_t^+-\delta_t^-} + \delta_t^- \frac{\delta_t^+}{\delta_t^+-\delta_t^-} =0$$
so the choice of delta and thus the sequence of fractional coloring produced by the algorithm before its termination is a martingale. This gives a sort of unbiasedness to the algorithm which is desirable when dividing elements into groups. It also means that we can tune the final outcome through the starting coloring.

One can see that $\textbf{v}^\perp _t$ is simply the last vector of the Gram-Schmidt orthogonalization of the ordered sequence of vectors $(\textbf{v}_i)_{i\in A_t}$. This is where the name of the algorithm comes from.

It is important to notice that $\textbf{v}^{\perp}_t$ depends on $t$ and not just on $\textbf{v}_{n(t)}$, as $A_t$ can change while the pivot $n(t)$ stays the same.

The update direction $\textbf{u}_t$ satisfying our conditions always exists because
\begin{align*}
        \textbf{v}^\perp_t = \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
        \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i = -(\textbf{v}_{n(t)} - \textbf{v}^\perp_t) \in V_t
\end{align*}

The vector $\textbf{u}_t$ is defined with $\textbf{u}_t(i)=0$ if $i$ is frozen, $\textbf{u}_t(n(t))=1$ and for the rest of the indices we have that :
\begin{align*}
\textbf{v}^\perp(t) &= \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
\Leftrightarrow \textbf{v}^\perp(t) &= 1 \cdot \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i + \sum_{i \notin A_t} 0 \cdot \textbf{v}_i\\
\Leftrightarrow \textbf{v}^\perp(t) &= \sum_{i=1}^n \textbf{u}_t(i)\textbf{v}_i
\end{align*}

$\textbf{v}^\perp_t$ will correspond to the direction that the output of the random walk will move in during time step t, ie if the matrix $\textbf{M}$ contains the input vectors as columns, $\textbf{v}^\perp_t = \textbf{Mu}_t$. So the algorithm is greedily optimal in the sense that the update direction is trying to add as little discrepancy as possible while moving towards a vertex of the hypercube.

$\textbf{u}_t = \arg\min_{\textbf{u} \in U} \|\textbf{Mu}\|$ where $U$ is the set of vectors in $\mathbb{R}^n$ such that $\textbf{u}(i) = 0 , i \not\in A_t$ and $\textbf{u}(n(t))=1$. This means that one can completely forget about $v_\perp$ and system solving and just use least square at each step to find the coordinates of the update directions that aren't alive or the pivot through the following formula $$u(A_t\setminus\{n(t)\})=\arg\min_u\|v_{n(t)}+\sum_{i\not\in A_t\setminus\{n(t)\}}u(i)v_i\|^2=(M_t^TM_t)^{-1}M_t^Tv_{n(t)}$$ where $M_t$ is the matrix containing all vectors that are alive but not the pvot as columns. In some case, when $v_\perp=\textbf{0}$ there are infinitely many soltions that minimize our objective function, so it would be interesting to add some conditions or do some quadratic programming to try to get a $u$ that has some desirable properties.

%\subsection{Examples}
%One in 3d, then the graph with all kind of different outputs on it
\begin{figure}[h]
\include{4types_3}
\caption{Plot of ouput sum of group assignments from the GSW, its input vectors, and a centered normal with $Cov=I_2$. There are 100 input vectors and 100 of each kind of point, everything is in dimension 2.}
\label{4types_3}
\end{figure}
\begin{figure}[h]
\include{4types_4}
\caption{Plot of ouput sum of group assignments from the GSW, its input vectors, and a centered normal with $Cov=I_2$ similarly to Figure \ref{4types_3}, except we added the output sums of random group assignments for comparison. There are 100 input vectors and 100 of each kind of point, everything is in dimension 2.}
\end{figure}
\begin{figure}
\begin{center}
\newpage
\multido{\i=0+1}{3}{
\includegraphics[width=12.7cm]{3d_example/gswalkboth\i.pdf}
}
\caption{Example of a GSW run in with 3 vectors in 2 dimension. The left part shows the cube where the coloring is living, and the right part shows the output sum and the input vectors.}
\label{3d_example}
\end{center}
\end{figure}
\subsection{Results}
\begin{definition}
A random vector $\textbf{Y} \in \mathbb{R}^m$ is said to be subgaussian with parameter $\sigma$ (or $\sigma$-subgaussian) if for all $\theta \in \mathbb{R}^m$:
$$\mathbb{E}[e^{\langle\textbf{Y},\theta\rangle}]\leq e^{(\sigma^2/2)\|\theta\|_2^2}$$ \\
\end{definition}
This definition will help us give a concrete result on the balancing power of the GSW. A random vector being 1-subaussian means that it is less spread that a gaussian with covariance matrix being the identity. We can see that for the output sum of the GSW, on Figure \ref{4types_3}, it seems to indeed be the case. This was indeed proved, first for $\sigma^2=40$ in \cite{blues} and then for $\sigma^2=1$ in \cite{harshaw2019balancing}.
\begin{theorem}[\cite{harshaw2019balancing}]
    For $\textbf{z}$ sampled via the Gram-Schmidt walk design, we have that $\textbf{Bz}$ is subgaussian with parameter $\sigma^2=1$:$$\mathbb{E}[exp(\langle\textbf{Bz},\textbf{v}\rangle)]\leq exp(\|\textbf{v}\|^2_2/2)\quad\forall\textbf{v}\in\mathbb{R}^{n+d}$$
\end{theorem}
This result is necessary to prove that the GSW indeed gives a constructive algorithm for Theorem \ref{banaszczyk}, by using Theorem \ref{equivalence}. 
%\begin{definition}
%We say that $L\in\mathbb{R}^{n\times n}$ is bounded in the Loewner order by $M\in\mathbb{R}^{n\times n}$, also written as $L\preceq M$ if $M-L$ is positive semi-definite, that is $z^T(M-L)z\geq 0$ $\forall z\in\mathbb{R}^n$.
%\end{definition}
%This other result from \cite{harshaw2019balancing} gives us a bound on the covariance of the vector of imbalances, which tells us that TODO
%\begin{theorem}[\cite{harshaw2019balancing}]
%If all input vectors of the GSW $\textbf{v}_1,\dots,\textbf{v}_n$ have $\ell_2$ norm at most 1, then the covariance
%matrix of the vector of imbalances $Bz$ is bounded in the Loewner order by the orthogonal projection onto the subspace spanned by the columns of $B$: $$Cov(Bz)\preceq P = B(B^TB)^\dagger B^T,$$
%where we recall that $M^\dagger$ is the pseudoinverse of the matrix $M$.
%\end{theorem}
\section{Fast computations}
Thanks to an original idea from professor Marcus, one can fasten the GSW from the trivial implementation which takes time $O(n^3d)$ down to $O(n^2d)$, similarly to what was done in \cite{harshaw2018balancing} except the proofs are easier. The basic idea is similar. At the beginning of a GSW run, we compute some matrices and then during each step of the algorithm, we use only rank one updates to maintain them and derive the update direction from them at each iteration.



For some set $S\subset\{1,\dots,n\}$, let $I_S\in\mathbb{R}^{n\times n}$ be the diagonal $n\times n$ matrix with $I_S(i,i) =1$ if $i\in S$ and $0$ otherwise, and $B_S\in\mathbb{R}^{d\times n}$ be $BI_S$ where $B$ is as previously the matrix containing the input vectors as columns. We define $C_S\in\mathbb{R}^{n\times d}$ to be such that $C_SB_S=I_S$ and if $i\not\in S$, the $i$th row of $C_S$ is $0$. Finally, $D_S\in\mathbb{R}^{n\times n}$ is $C_SC_S^T$.

This technique necessitates that the dimension of the space spanned by the input vectors is at most the dimension $d$ of the input vectors, as otherwise $C_S$ doesn't exist. %For the other case, one could produce another technique.

Here are the rank one update that let us compute ou matrix for some $S'=S\setminus\{k\}$.
\begin{lemma}\label{update_lemma}
For $k\in S$, let 
\begin{itemize}
\item $c_k\in\mathbb{R}^{1\times n}$ be the $k$th row of $C_S$
\item $d_k\in\mathbb{R}^{1\times d}$ be the $k$th row of $D_S$
\item $S'= S\setminus\{k\}$
\end{itemize}
Then $C_{S'}=C_S-\frac{1}{d_k(k)}d_k^Tc_k$ and $D_{S'}=D_S-\frac{1}{d_k(k)}d_k^Td_k$.
\end{lemma}
\begin{proof}
For the first part, we have that\begin{align*}
\left(C_S-\frac{1}{d_k(k)}d_k^Tc_k\right)B_{S'}&=\left(C_S-\frac{1}{d_k(k)}d_k^Tc_k\right)B_{S}I_{S'}\\
&=C_SB_SI_{S'}-\frac{1}{d_k(k)}d_k^Tc_kB_{S}I_{S'}\\
&=I_{S'}-\frac{1}{d_k(k)}d_k^Te_kI_{S'}\\
&=I_{S'}-\frac{1}{d_k(k)}d_k^T\textbf{0}\\
&=I_{S'}\end{align*}
Thus we just need to show that the rows of $C_S-\frac{1}{d_k(k)}d_k^Tc_k$ not in $S'$ are 0. As $\frac{1}{d_k(k)}d_k(k)=1$, we have that the $k$th row of $C_S-\frac{1}{d_k(k)}d_k^Tc_k$ is indeed 0. Lastly, as $D_S=C_SC_S^T$, we have that $i\not\in S\Rightarrow d_k(i)=0$ thus all the rows whose indices aren't in $S$ indeed do stay 0.

For the second part, we can see that \begin{align*}
D_{S'}&=C_{S'}C_{S'}^T\\
&=\left(C_S-\frac{1}{d_k(k)}d_k^Tc_k\right)\left(C_S-\frac{1}{d_k(k)}d_k^Tc_k\right)^T\\
&=\left(C_S-\frac{1}{d_k(k)}d_k^Tc_k\right)\left(C_S^T-\frac{1}{d_k(k)}c_k^Td_k\right)\\
&=C_SC_S^T-\frac{1}{d_k(k)}C_Sc_k^Td_k-\frac{1}{d_k(k)}d_k^Tc_kC_S^T+\frac{1}{d_k(k)^2}d_k^Tc_kc_k^Td_k\\
&=D_S-\frac{1}{d_k(k)}d_k^Td_k-\frac{1}{d_k(k)}d_k^Td_k+\frac{1}{d_k(k)}d_k^Td_k\\
&=D_S-\frac{1}{d_k(k)}d_k^Td_k\end{align*}\end{proof}
Now let's see why this is useful, that is, how to compute $v^\perp(t)$ and $u_t$ from these matrices.
\begin{lemma}
Assume we're currently in a step of the GSW where the active vectors are $A_t=\{v_s:s\in S\}$, and $v_k\in A_t$ is the pivot. If $d_k$ and $c_k$ are defined similarly as in Lemma \ref{update_lemma}, we have that $$v^\perp(t)=\frac{c_k}{\|c_k\|^2}\textrm{ and }u_t=\frac{d_k}{d_k(k)}.$$
\end{lemma}
\begin{proof}
$c_k$ is perpendicular to all vectors but $v_k$ as $c_kv_i^T=0$ $\forall i\not=k$. Thus we just need to make sure
\end{proof}
These lemma mean that once we have computed $C_S$ and $D_S$ at the beginning of the GSW in time $O(n^2d+n^3)$, we can update them in time $O(nd+n^2)$ and store them with a similar quantity of memory, resultng in a total time of running of $O(n^2(d+n))$ as computing $\delta_t$ and updating the coloring takes $O(n)$ time.


\section{Generalizing to any hyperparallelepiped}
The idea is to allow the algorithm to sample coloring not only on the hypercube $[-1,1]^n$, but also on any set $b_1,\dots, b_n$ spanned by $n$ linearly independent vectors in $\mathbb{R}^n$. It then won't be a coloring per se but just a linear combination of the vectors with some coefficients corresponding to a vertex of the hyperparallelepiped formed by the basis vectors. We will still denote it by the term coloring for clarity reasons.

To do so, we have to adapt the algorithm at several points. Firstly, an element $k$ associated to the basis vector $b_k$ should be alive only if the current coloring is not on one of the 2 facets such that when expressed in the basis $b_1,\dots,b_n$, the $k$th coordinate is -1 or 1. 

Secondly, choosing the update direction is then different as we need to stay orthogonal from every fixed vector, but these vectors don't correspond to coordinates anymore.

Thirdly, the computation of the two potential $\delta$ is different as well for some similar reasons.

All these issues can be solved via some basis changes in the right spots and the modified algorithm is very similar, and should work exactly the same way when given the orthonormal canonical basis of $\mathbb{R}^n$.

%Explain the changes, give pseudocode, give outputs.
\section{Generalizing to more than two groups}
Discrepancy minimization is generally set in a 2-group paradigm, but it could be interesting to generalize the GSW to separate into more than 2 groups. For example, if one wanted to separate $n$ vectors in 3 groups, the GSW could first be used to separate into groups $G_+,G_-$ such that $\sum_{v\in G_+}v-2\cdot\sum_{v\in G_-}v\approx \textbf{0}$, by starting at $x_0=(1/3,1/3,\dots,1/3)\in\mathbb{R}^n$. Then the group $G_+$ could be again inputed into the GSW to separate it into $G_{++}$ and $G_{+-}$, and we would expect $G_-,G_{++} and G_{+-}$ to be roughly balanced mutually but also all together.

But how do we know if a 3 group assignment $G_i$ for $i\in\{0,1,2\}$ is balanced ? For 3, one could use the complex roots of 1, $\omega_0=1 ,\omega_1$, and $\omega_2$, and check that $$\sum_{i\in\{0,1,2\}}\omega_i\sum_{v\in G_i}v\approx\textbf{0}.$$

One issue it that this seems like a bandaid method, and it seems like it would yield before grou assignment to have an algorithm that separates in $m$ groups from the get go.

Another issue is that this doesn't generalize to a higher number of groups, which is why seeing we need another perspective. One  idea is to link each group with a vertex of the $(m-1)$-dimensional regular simplex centered in \textbf{0} where $m$ is the number of groups we want to separate our vectors into. So we would want to assign each group to a vertex and verify that the sum of our vectors is close to \textbf{0} in each of the $m-1$ dimensions. This would mean that our coloring would live in $S_{m-1}^n$, each vector moving in its personal copy of the simplex until it gets fixed to one of the vertices.

We would have to adapt the choice of the update direction and the choice of $\delta$ which could maybe also be multi-dimensional. One big issue then is that choosing an update direction is far from obvious. Should we force the multidimensional vector of update of the pivot to be of norm 1 ? If so how to choose it ? Additionally, assuming we have an update direction, what should one do when the border of the simplex is hit but not the vertex ? Should we now force the coloring to stay fixed to that border and now move in that border ? Should we choose the update direction and $\delta$ in a way that borders are never hit outside of vertices ?

All these questions are tough to answer, and generalizing the GSW to separate in $m$ groups would require understanding them deeply. Sadly, I did not succeed in finding such a generalization.

\section{Experiments and Properties}
By balance of the assignment, we mean the difference between the number of 1s and -1s. An assignment is perfectly balanced if its sum is 0.

\subsection{How good is the GSW at minimizing output discrepancy in practice ?}
We're interested in seeing how well does the GSW actually perform in minimizing the norm. We will compare it to the naive walk defined in \ref{}, the deterministic GSW \ref{} and the actual best computed via bruteforcing.

\subsubsection{Experiment}\label{how_good_at_minimizing_disc}
We compare the output discrepancy of GSW, DGSW, the naive walk and for some small $n$ also the best assignment found via brute forcing on all possibilities. We do this for$n=5,10,15,20$, and with $d=2^i$ for $i\in\{1,\dots,15\}$, where we sample $n$ vectors from the $d$-dimensional ball of radius 1.

\subsubsection{Results}
\begin{figure}
\centering
\include{comparative_norms_n=80_repeat=1000_max_dim=32768.0}
\caption{Comparison of different discrepancy minimizing vectors for $n=5,10,15$ and $20$ vectors of dimension up to $2^{15}$. Results were averaged over 1000 runs.}\label{output_disc}
\end{figure}
Our results are visible in Figure \ref{output_disc}. We can see that GSW actually gives the worst results in terms of discrepancy minimization, but that when the dimension of the vector grows all methods seem to give similar results asymptotically. Note that we cannot say that the naive walk is just a better discrepancy-minimizing algorithm as these results would probably be different if we modified the distribution of input vectors.

\subsection{Does translation affect the balance of the assignment ?}\label{trans_balance}
If your initial group of vector is centered around 0, we would expect that translating it away will force it to have a greater balance between -1s and 1s in order to balance the translation part added to each vector.

\subsubsection{Experiment}
We sample 200 input vectors from the ball in dimension 200. We then run the GSW and DGSW 100 times each on those vector translated by some random norm 1 vector multiplied by some factor. We use the factors 0,1,2,5 and 10 and compare the results.

\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 Factor & 0 & 1  & 2 & 5 & 10  \\
\hline
GSW  & 9.7 & 0.96 & 0.46 & 0.1 & 0.06 \\
DGSW & 44.5 & 1.78 & 0.32 & 0.08 & 0.04
\end{tabular}
\caption{Results of our experiment on the balance of assignments depending on how much the vectors are transated. The numbers shown are the average absolute value of the sums of output vectors. A smaller number indicates that the assignment has a more balanced assignment, that is the number of 1s and -1s are closer.}
\label{balance_when_translated}
\end{table}
\end{center}
We can see that indeed the further away from 0 our input vectors are translated the more balanced the assignments are, as expected. It's interesting to notice that assignments from the DGSW are way less balanced than with the GSW. These experiments make us want to try to build a variant of GSW that can have a balance parameter thanks to the balancing properties of translation. That is what we try in the following experiment.

\subsection{A parameter to balance assignments}\label{balance_parameter}
Inspired by section \ref{trans_balance}, we propose a slight modification of the GSW that pushes toward balanced assignments. The idea is to add a coordinate to the input vector and give more or less importance to that coordinate similarly to how the balance-robustness tradeoff is implemented in \cite{harshaw2019balancing}, except here we implement a tradeoff between assignment balance and output balance.

Given input vectors $v_1,\dots,v_n\in\mathbb{R}^d$ and a parameter $\mu\in[0,1]$, we define $w_1,\dots,w_n\in\mathbb{R}^{d+1}$ as $$w_i=\begin{pmatrix}\sqrt{1-\mu}v_i \\ \sqrt{\mu}\end{pmatrix}.$$ This way the $w_i$'s have similar norm to the $v_i$'s, but maybe a different normalization depending on the norm of the $v_i$'s could be better. We then run the GSW on them and use the output assignment on the original vectors. Choosing $\mu=0$ is equivalent to doing the classical GSW algorithm, while using $\mu=1$ is equal to forcing exact balance. We run experiments to determine how much balance in the assignment we gain and how much further from \textbf{0} our output is for different $\mu$s.

\subsubsection{Experiment}
We use our just explained construction to compute an assignment on the $w_i$'s using GSW or DGSW, then see how this assignment performs on the original $v_i$'s in terms of balance of the assignment and discrepancy. We also program the fixed size GSW described in \cite{harshaw2019balancing} and compare it. We use $\mu\in\{0,0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999,1\}$, and $n$ vectors sampled from the ball of radius 1 in dimension $n$ for $=100$. The results presented are averaged over 1000 runs.

\subsubsection{Results}

\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllllllllll}
 $\mu$ &0&0.001&0.01&0.1&0.25&0.5&0.75&0.9&0.99&0.999&1&FSD \cite{harshaw2019balancing}\\
\hline
AB&8.086&6.248&4.164&1.898&1.108&0.576&0.266&0.106&0.01&0.002&0&0 \\
AD&5.259&5.275&5.266&5.311&5.341&5.321&5.336&5.331&5.334&5.337&9.917&5.317\\
ABD&20.584&18.698&11.908&3.886&2.044&0.994&0.328&0.116&0.018&0.002&0&0\\
ADD&4.863&4.837&4.775&4.821&4.885&4.938&4.991&5.004&5.008&5.002&9.851&5.01
\end{tabular}
\caption{Results of our experiment on the balance of assignments with our new balance-discrepancy tradeoff design. The balance numbers shown (lines starting with AB for average balance) are the average absolute value of the sums of output vectors, and the discrepancy numbers shown (lines starting with AD for average discrepancy) are the norms of the sum of $v_i\cdot x_i$, $x$ being the assignment produced by GSW for the $w_i$'s except for the last column in which we used the fixed size GSW design from \cite{harshaw2019balancing}. The D at the end of the last two lines indicates that in this case, the deterministic GSW algorithms was used. \\A smaller balance number indicates that the assignment has a more balanced assignment, that is the number of 1s and -1s are closer. A smaller discrepancy number indicates that the vectors are better balanced among the groups, that is the sum of each coordinate is closer to be the same in each group. FSD \cite{harshaw2019balancing} references the fixed size design from \cite{harshaw2019balancing}.}
\label{balance_tradeoff_results}
\end{table}
\end{center}

We can see that we can massively increase assignment balance without making the output norm much larger. For $\mu=0.999$ for example, it is very likely that an assignment is exactly balanced and thus this variant of the algorithm can provide balanced GSW assignments with high probability while keeping all properties of the classical GSW, as opposed to the balanced variant described in \cite{harshaw2019balancing}, for which we don't know if some of the original properties still hold. The high probability comes from the subgaussianity bound.

\subsection{What else can we control by adding coordinates ?}
We could think of a design where we want several subgroups, potentially of only 2 vectors per subgroup, to be balanced. We could then add a coordinate for each subgroup and put the same number on that coordinate for each member of the subgroup, while putting 0 for every vector that isn't in the subgroup. This would push towards balancing within any subgroup we want, and could be done via some parametering similar to what was done in section \ref{balance_parameter}. 

Similarly, if we want 2 vectors to be in the same group, we could add a dimension and assign some number $x$ and $-x$ to these vectors in that new dimension while giving 0 to every other vector in that dimension. Adding dimension could be used to translate knowledge about the vector set into usable information for the algorithm.



\subsection{Does norm affect the balance of the assignment ?}
I would expect the norms not to affect the balance of the assignment, as multiplying every input vector by the same vector should mean the algorithm runs similarly.

\subsection{Does norm affect when a vector is colored ?}\label{norm_affect_when}
The expected heuristic would have been that bigger vectors are colored earlier and the algorithm then colors the smaller ones to minimize discrepancy as that is what I'd instinctively do. It turns out that the algorithm actually does the reverse and colors the smaller vectors earlier than the bigger ones.

We performed two experiments to observe this behavior. In both experiments, we have vectors $v_1,\dots,v_{200}$ of increasing norm and observe how close the coloring order is to $\mathcal{R}=\{200,199,$\dots$,1\}$. To do so, if $\mathcal{O}$ is the observed order, we look at the quantity 
\begin{equation}
\Delta_{o}=\sum_{i=1}^{200}|\mathcal{R}_i-\mathcal{O}_i|.
\label{orderdistance}
\end{equation}
The smaller it is, the closer the 2 orders are. For each of the two experiments below, we ran the GSW 100 times and the deterministic GSW (DGSW) 100 times and recorded $\Delta_o$.

\subsubsection{First Experiment}
We sampled 200 vectors $\textbf{v}_1,\dots,\textbf{v}_{200}$ in the ball of radius 1 of dimension 200. For each $v_i$, we replaced it by $i\cdot \textbf{v}_i/\|\textbf{v}_i\|$ so that $\|\textbf{v}_i\|=i$ for each vector. 

\subsubsection{Second Experiment}
We sampled 200 vectors $\textbf{v}_1,\dots,\textbf{v}_{200}$ in the ball of radius 1 of dimension 200. For each $v_i$, we replaced it by $X_i\cdot \textbf{v}_i/\|\textbf{v}_i\|$ so that $\|\textbf{v}_i\|=X_i$ for each vector, where $X_i=1$ if $i<n/2$ and 200 otherwise.

\subsubsection{Results}
We also ran the same experiments with 200 vectors of constant norm as a comparison. The results are summarized in Figure \ref{norm_when_colored}.
\begin{center}
\begin{table}[h]
\begin{tabular}{l|lll}
     & Exp 1   & Exp 2    & Control  \\
\hline
GSW  & 18664.4 & 19997.32 & 13607.06 \\
DGSW & 18641.6 & 19996.16 & 13431.68
\end{tabular}
\caption{Result of our experiments on the moment of coloring depending on the norm of the vectors. The numbers shown are the $\Delta_o$ as defined in equation \ref{orderdistance}.}
\label{norm_when_colored}
\end{table}
\end{center}
We can see that the vector orders are actually further away from $\mathcal{R}$ than the random orders produced with constant vectors, which means that bigger vectors actually get colored later in the process. Additionally, the DGSW doesn't seem to yield significantly different results. While this is not what I expected, this behavior actually makes sense, because if we multiply $\textbf{v}_i$ by $\mu\textbf{v}_i$, it's corresponding coordinate in $\textbf{u}$ is going to be divided by $\mu$, thus it will move less towards the border of the hypercube and thus be colored later than the shorter vectors.

This motivates us to try to find a variant of the algorithm that would color bigger vectors earlier and thus perform better on an input such as $\{v,v,v,3v\}$ for $v\in\mathbb{R}^d$ for an arbbitrary $d\in\mathbb{N}$. One way would be to choose the pivot through some smart condition or to choose another feasible $\textbf{u}$ than the default least square solution with minimal norm, again through a smart criterion.

One such idea is to force the pivot to be the largest norm vector, hen, when $v_\perp=\textbf{0}$, to select $u_t$ through lasso with a very small alpha ($\alpha=10^{-32}$ for example) in order to ensure that the smallest number possible of coordinates are nonzero, and finally to select $\delta_t$ by taking it to be of the same sign as the coordinate of $x$ corresponding to the pivot (or randomly if that coordinate is 0). This variant solves the issue mentioned in the previous paragraph but loses a lot of randomness in the process, so there probably exist some different additional constraint to add when computing $u_t$ in colinear cases that could work even better.

Another variant that works but only in this trivial example and not in slightly more complicated examples with for examples 2 groups of vectors is to just force the largest alive vector to be the pivot at every step. This is just a product of the solution of the least squares we're choosing as there are infinitely many that wouldn't work.

A third variant that might help is to do quadratic programming instead of simple least squares when $v_\perp=\textbf{0}$. This way, we can force the quadratic program to minimize both $\|Bu\|$ but also $\|u\|$. This could be achieved by adding a line of 1's to the matrix $B$ containing all input vectors as column, and adding as conditions that the chosen $u$ must have a 1 in the pivot coordinate and 0s in already colored coordinates. Then if the $u$ chosen through this quadratic program doesn't yield $Bu=\textbf{0}$, we discard it and compute $u$ through the usual method. This technique coupled with choosing the longest alive vector as pivot actually solves trivial adversarial cases, and doesn't modify too much the algorithm. The only small issue is that the matrix $B^TB$ then needs to be regularized by adding some very small constant times the identity or it isn't positive definite.

\subsection{What affects when a vector is colored other than the norm ?}

\subsection{Do longer vectors stay pivot for longer ?}\label{longer_vec_pivot_longer}
As longer vectors are colored later in the algorithm on average, one could think that they're staying as the pivot for longer. To test this hypothesis we design the following experiment.
\subsubsection{Experiment}
We sample 200 vectors of norm 1 in dimension 200 and multiply 100 of them by 200 (G1 with norm 1, G2 with norm 200). We then run the GSW 100 times with all these vectors as input and record for how long vectors of different norms (1 and 200) stay pivot once the become pivot.
\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll}
 &ALOSAP G1&ALOSAP G2\\
\hline
GSW&6.101&2.281\\
DGSW&5.989&2.277
\end{tabular}
\caption{Result of our experiments on whether long and short vectors stay for longer as the pivot. ALOSAP stands for average length of stay at pivot and is measured as the average number of steps over 100 runs.}
\label{pivot_longer}
\end{table}
\end{center}
Results are visible in Table \ref{pivot_longer}. We sede that shorter vectors tend to stay pivot for much longer than their longer counterparts. This could be explained by the fact that coordinates of the update direction for all long vectors are very small so longer vectors rarely get colored by a small vector pivot, but shorter vectors do get colored while the longer vectors are pivot.

\subsection{Can we force bigger vectors to be colored earlier ?}
Another technique that we could use would be to modify the choice of the direction. Currently, the bigger a vector is the later in the process it will be colored. One could multiply the computed direction in each of its coordinate by the norm of the vector corresponding to that coordinate, or the squared norm. %maybe add some reasoning why we might want to do that
This would remove the orthogonality of updates, but in practice it didn't seem to change significantly how far the sum of outputs were from \textbf{0}. We can study how that affects the order of the coloring in experiments similar to those done in subsection \ref{norm_affect_when}.

\subsubsection{Experiments}
We sample vectors similarly to the experiments performed in subsection \ref{norm_affect_when} and measure similarly how close the coloring order is to the order $\mathcal{R}=\{200,199,$\dots$,1\}$.
\subsection{Results}
We also perform the same experiments with a group of constant norm vectors
\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 &E1 with $D$&E1 with $D^2$& E2 with $D$&E2 with $D^2$& Control with $D$&Control with $D^2$   \\
\hline
GSW&13246.72&6078.28&13246.74&6697.1&13444.94&13208.1\\
DGSW&6808.96&13100.02&13597.86&6830.82&13303.18&13344.14
\end{tabular}
\caption{Result of our experiments on trying to fix the later coloring of bigger norm vectors. The numbers shown are the $\Delta_o$ as defined in equation \ref{orderdistance}.}
\label{norm_earlier}
\end{table}
\end{center}
We can see that our modifications indeed remove the late coloring. Multiplying once makes it so the vectors are colored approximately randomly and the multiplying twice makes it so the bigger vectors are colored earlier, as intended. There are very likely other ways of getting a similar effect, potentially by adding coordinates smartly.

\subsection{Can we find another way of computing the update direction ?}
 As we saw that larger vectors get colored later in the process on average when using the classical algorithm, one could ask themselves how to revert this effect. Let $A$ be the matrix containing our input vectors as columns. Using a singular value decomposition, we have that $A=U\Sigma V^T$ where $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ are orthonormal and $\Sigma\in\mathbb{R}^{m\times n}$ is all zeros except for the diagonal elements which are positive singular values. Using this decomposition, we can see that $A^+=V\Sigma^+U^T$ where $\Sigma^+$ is $\Sigma^T$ except nonzero entries $\sigma_i$ are replaced by their inverse $1/\sigma_i$. But if one replaced $\Sigma^+$ by $\Sigma$, then the  matrix multiplying the pivot vector would be $A^T$ instead of $A^+$. This suggestion from Pr. Marcus turned out not to work if we want to balance the vectors, but it actually does the opposite which is very interesting. 

\subsubsection{Experiment 1}\label{exp1_A_T}

We sampled 200 vectors in dimension 200 in the ball of radius 1. We then ran the modified GSW algorithm where the next direction is computed via $u_t(\mathcal{A}_t\setminus\{p(t)\})=B_tv_{p(t)}$ and $u_t(p(t))=1, u_t(i)=0 \forall i\not\in\mathcal{A}_t$. Everything else is kept similar. 

\subsubsection{Experiment 2}\label{exp2_A_T}


We run as similar experiment as in \ref{how_good_at_minimizing_disc} except we look for the discrepancy maximizing assignment via bruteforcing and look at the naive walk trying to maximize the output norm. We also do not go as high in the dimension in order for the experiment to run faster.

\subsubsection{Results}
\begin{figure}[h]
\centering
\include{A_T_instead_of_lstsq_200}
\caption{Results of Experiment 1 (Section \ref{exp1_A_T}). Output sums using the modified GSW and DGSW where the update direction is computed by multiplying the pivot vector by $A^T$.}
\label{A_T_instead_of_lstsq}
\end{figure}
The outputs are shown in the figure \ref{A_T_instead_of_lstsq}. We can see that this modification seems like it now minimizes output balance instead of maximizing it, which was surprising to me at least. This seems like it could be useful to sample from unbalanced group assignments, or to find a subset to remove to maximize something. This might be equivalent to an already known algorithm, but if not I think there are probably interesting applications of this.

\begin{figure}[h]
\centering
\include{comparative_norms_n=20_repeat=100_max_dim=8192.0_maximizing}
\caption{Results of Experiment 2 (\ref{exp2_A_T}). Comparing the modified GSW and DGSW with discrepancy maximizing algorithms.}
\label{A_T_instead_of_lstsq_2}
\end{figure}
We can see that using $A^T$ doesn't actually maximize the norm for small dimensions even though it seems to asymptotically do so when the dimension grows. It would be interesting to investigate the evolution of this phenomenon when $n$ grows as the output norm seems to go down then back up for $n=15$ and $20$.

\subsection{Are vectors with smaller dimensionality colored at the same moment as vectors with more dimensions ?}\label{smaller_dimensionality}
We want to know if vectors with a lot of 0's can be found among vectors that are less sparse, as that could be very interesting to solve various problems such as the planted clique. We will investigate how early they're colored on average.

\subsubsection{Experiment}
We sample 100 vectors of dimension 100 from the ball radius 1 but with 100 additional coordinates locked to zero. We also sample 100 vectors of dimension 200 from the ball of radius 1. We're interested in comparing whether vectors in one group get colored earlier than vectors from the other group on average. To do so we do 100 runs with each of three variants. In the first variant (V1), the vectors aren't changed. In the second one (V2), every vector is normalized. In the third one (V3), every vector is normalized but the non-sparse vectors are normalized to a norm of 2 so that the scale of the elements are similar to the sparse vectors normalized to a norm of 1. The last three variants are respective copies of the first three except the coordinates of the sparse vectors are shuffled so that the 0's aren's uniformly placed in the sparse group.

\subsubsection{Results}
%Table
\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 &V1&V2&V3&V4&V5&V6\\
\hline
GSW&75.781&75.281&60.726&98.163&100.268&68.584\\
DGSW&75.393&76.594&61.692&97.231&98.883&68.088
\end{tabular}
\caption{Result of our experiments on whether sparse vectors are colored earlier. The numbers shown are the average coloring step of sparse vectors over 100 runs of the GSW.}
\label{pivot_longer}
\end{table}
\end{center}
We can see that sparse vectors are colored much earlier in the first 3 variants, and even earlier in the third variant, which might be explained also by their smaller norm relative to the non-sparse vectors. The last three variant show use that the earliness effect seems to be nearly completely linked to the fact that the sparse vector were not shuffled in the first three variants, as the sparse vectors are colored very close to the average of 100.

%Additional ideas
It could also be interesting to investigate how balanced each vector group is and how noise affect this result, as this could help us discover a hidden group in a larger set. Another interesting thing would be to see how varying the relative size of the two groups affects the phenomenon. Lastly, 
%We could also just measure this for sparse vectors that do not necessarily all have 0's on the same dimensions. DONE

\subsection{How often is the pivot vector colored ?}\label{pivot_colored}
It would be interesting to know how often the pivot is colored, and whether it depends on the pivot choice rule, or just on the vector set. To do so we try a couple different choice rules and vector set and investigate.
\subsubsection{Experiment}
We use three pivot choice rules: the \textbf{random} (R) rule, that is the classic one where the pivot is cosen uniformly at random when a new pivot is needed, the \textbf{maximum norm} one (MN) where the pivot is always chosen as the vector that has the greatest norm among all vectors that are alive, and the \textbf{norm-proportional} (NP) rule that makes it so each vector that is alive has a probability of being picked as the next pivot that is proportional to its norm. 

We use three different sets of 100 vectors in dimension 2,10 and 100, which correspond to those used in section \ref{norm_affect_when},that is the all equal norm set (AEN), the two group set (2G) where half the vectors have size 1 and the other half size $n=100$, and the incrementally growing norm set (IGN) where the vectors have respectively norms 1,2,...,100. We observe the proportion of time steps during which the pivot is colored among all time steps and try to see whether the pivot rule seems to make that proportion vary or whether it only depends on the vector set.

\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|lll|lll|lll}
Dimension &\multicolumn{9}{c}{\textbf{2}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.958&0.949&0.939&0.920&0.474&0.505&0.946&0.912&0.892\\
DGSW  &0.976&0.969&0.952&0.944&0.504&0.509&0.971&0.940&0.914\\
\hline
\hline
Dimension &\multicolumn{9}{c}{\textbf{10}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.822&0.798&0.822&0.657&0.357&0.346&0.761&0.578&0.628 \\
DGSW  &0.870&0.841&0.862&0.705&0.383&0.374&0.811&0.638&0.673\\
\hline
\hline
Dimension &\multicolumn{9}{c}{\textbf{100}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.532&0.527&0.537&0.434&0.438&0.420&0.463&0.362&0.421 \\
DGSW  &0.521&0.538&0.525&0.424&0.445&0.431&0.461&0.387&0.441\\
\end{tabular}
\caption{Result of our experiments on how often is the pivot colored as a function of the pivot choice rule and the input vector set. Results are averaged over 100 runs of the GSW each and are proportions between 0 and 1.}
\label{pivot_colored_results}
\end{table}
\end{center}
Results are available in Table \ref{pivot_colored_results}. Notice that when using the maximum norm mode with DGSW we lose all randomness, which is probably undesirable but these were still included for completeness.

The DGSW proportion all seem a little higher in the dimension 2 and 10 cases, while in dimension 100 there seems to be no significant differences. So coloring the closest point on the update direction seems to push toward coloring the pivot most of the time in smaller dimensions, but when the dimension grows this effect seems to vanish

As the vectors only have 2 potential different norms that are very different in size, we would expect the 2G set to not have much difference across the maximum norm and norm-proportional pivot choice rule, and that is indeed the case.

We can also see that the pivot rule does not seem to change the pivot coloring rate significantly in the AEN case.

We can see on the 2G and IGN vector sets that the proportion of colored pivots is lower when using non-uniformly random pivot rules which favor longer vectors, which is consistent with the observation that longer vectors get colored later from \ref{norm_affect_when}. This is absent on the AEN vector set as the norms are all equal.

Finally, the main observation seems to be that the pivot is colored more often when the dimension is small, which I currently don't have a logical explanation for.

\subsection{What if we choose the pivot as a function of the fractional coloring ?}\label{pivot_coloring_rules}
One could think of seeing the GSW as a rounding algorithm of some sort, and then, as the pivot is often colored as seen in Section \ref{pivot_colored}, we could want that the vectors closest to being colored are pivot as they're closer to being rounded. The pivot choice rule is also an interesting place to seek to optimize as it doesn't play a role inthe analysis from \cite{blues} and only plas a very minor role in section 6.2 of \cite{harshaw2018balancing}. Thus it leaves a lot of freedom that we can try to exploit to improve the GSW.

\subsubsection{Experiment}\label{pivot_colored_coloring_rules}
We first reproduce the experiment in Section \ref{pivot_colored} but with 2 new pivot choice rules: the \textbf{maximum absolute coloring} (MAC) rule that chooses as pivot the alive vector that is closest to -1 or 1 and separates ties uniformly randomly, and the \textbf{coloring proportional} (CP) rule that chooses the pivot according to a distribution such that the probability that an alive vector is chosen as the pivot is proportional to the absolute value of its current fractional coloring value, and choses uniformly randomly if the fractional coloring value of every alive vector is 0.

We would expect these methods to have higher pivot-coloring rates as the previous ones, as they're literally choosing elements that are more likely than average to be colored in the next step.
\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll|ll|ll}
Dimension &\multicolumn{6}{c}{\textbf{2}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.950&0.945&0.590&0.550&0.939&0.900\\
DGSW  &0.965&0.968&0.600&0.566&0.954&0.927\\
\hline
\hline
Dimension &\multicolumn{6}{c}{\textbf{10}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.889&0.840&0.406&0.386&0.758&0.711\\
DGSW  &0.930&0.887&0.444&0.419&0.798&0.769\\
\hline
\hline
Dimension &\multicolumn{6}{c}{\textbf{100}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.834&0.595&0.697&0.532&0.800&0.535\\
DGSW  &0.884&0.623&0.722&0.542&0.825&0.544\\
\end{tabular}
\caption{Result of our experiments on how often is the pivot colored as a function of the pivot choice rule and the input vector set, now with coloring-dependent pivot choice rules. Results are averaged over 100 runs of the GSW each and are proportions between 0 and 1.}
\label{pivot_colored_coloring_rules_results}
\end{table}
\end{center}
Results are visible in Table \ref{pivot_colored_coloring_rules_results}. We can see that as predicted, the pivot coloring rates are higher in this experiment than in section \ref{pivot_colored}, but they actually aren't for the 2 groups (2G) vector set using the random pivot. One potential explanation is that the longer vectors are often close to being colored but still harder to color than the small ones despite that, where the random pivot rule actually chooses a smaller vector 50\% of the time. 

For dimension 100 we see that the maximum absolute coloring rule actually yields a much higher pivot coloring rate than its proportional variant, which could be an indication that the fractional coloring become less sure, that is closer to 0 on average, the higher the dimension goes.

We can also notice that the DGSW leads to a higher coloring rate, which makes sense as we choose a pivot that's close to being colored and the DGSW will thus finish coloring it more often that the classical GSW.

\subsubsection{Experiment 2}\label{exp_plot_max_col}
We sample 100 vectors of dimension 2 and run both the classical GSW with random pivot and the classical GSW with \textbf{maximum absolute coloring} (MAC) similar to Section \ref{pivot_colored_coloring_rules}. We do similarly but for the DGSW too. We do one run per variantand per starting pivot and plot the results to see whether there is some pattern and how it looks like, similarly to figure \ref{4types_3}.

\subsubsection{Results 2}
\begin{figure}[h]
\centering
\include{comparative_norms_n=80_repeat=1000_max_dim=32768.0_max_col}
\caption{Results of Experiment 2 (Section \ref{exp_plot_max_col}). 100 output sums in dimension 2 using GSW, DGSW with both the random pivot rule and the maximal coloring rule.}
\label{results_plot_max_col}
\end{figure}
Results are visible in Figure \ref{results_plot_max_col}. We can see that as previously, the vector of imbalance from DGSW run are slightly closer to 0 than those from GSW runs. Additionally, we can see that the MAC pivot rule seems to give us shorter vector of imbalances as well, with MAC DGSW runs being visibly closer to \textbf{0}. While this effect could be expected as we color the vectors that allow us to pick smaller deltas and thus move less away from \textbf{0}, it is actually very visible here and we will investigate it further in the next experiment.

\subsubsection{Experiment 3}

\subsubsection{Results 3}

\subsection{Can we force the algorithm to color the pivot vector ?}
\subsection{What makes it so a vector is colored early outside of its norm and dimensionality ?}
%\subsection{Can we make the algorithm faster with some clever factorization ?}
\section{Finding structured subgroups}
We will try several experiments aiming to identify structured subgroups hidden in sets of random vectors. The idea is that the GSW should color members of the subgroup early as they should be easier to balance, and we will look at when the GSW colors which vectors over a large number of runs of the GSW and with various sets of vector and configurations. The ideal case would be if the GSW could help us solve a problem such as the Planted Clique.

This section is inspired by results such as those in section \ref{smaller_dimensionality}.

One problem that we would like to solve is, given $n-k$ vectors in a ball of radius 1 in some dimension $d$ and $k$ vectors in the same ball such that all their coordinates are positive, we apply a random rotation to the $k$ vectors. We would then like to use the GSW to find these vectors, by hoping that they would be colored early. We call this problem the \textbf{hidden cluster}.

One way to do so would be to look for the vector that's colored the earliest on average after some number$r$ of runs, and return that vector and the $k-1$ vector that are the closest to it. There are lots of moving parameters here, including $n,k,d$ and the specific GSW variant used. We will investigate when that approach works and when it doesn't. We call this technique the \textbf{early coloring}.

\subsection{How many runs are needed to get a decent idea of when a vector is colored ?}
In order to investigate that, we created some instances of the hidden cluster with $n=200,k\in\{5,10\},d=200$. We then used our early coloring method several times with $r=200$ and saw if the results differed over 5 runs for each set. As the end results were the same for each time we ran the method independently of which run it was, we concluded that $r=200$ was enough to get a good approximation of the average coloring times. 

This does not mean that the average coloring time of each vector were very close for each method run, but it means that the earliest vector was always the same.

\subsection{Does the early coloring technique work ?}
We want to know if the technique makes sense and works, so we created 50 instances of the hidden cluster with $n=200,k\in\{5,10,15\},d=200$, (so 50 for each $k$) and tried our technique with GSW, DGSW, MAC GSW and MAC DGSW, where MAC is defined in section \ref{pivot_colored_coloring_rules}. We then used our early coloring method with $r=200$ and recorded how often the technique successfully identified the hidden cluster. For DGSW with MAC which is actualy fully deterministic given the first pivot assuming random vector input, we made sure to use each of the 200 vectors as starting pivot exactly once.
\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll|ll|ll}
$k$  & \multicolumn{2}{c}{5} & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{15}\\
Rule &R&MAC&R&MAC&R&MAC\\ \hline
GSW  &0.26&0.28&0.34&0.52&0.66&0.7\\
DGSW &0.2&0.2&0.54&0.52&0.74&0.62\\
\end{tabular}
\caption{Result of our experiments on the early coloring method to solve the hidden cluster problem. The results are the proportion of instances for which the early coloring method worked out of 50 runs for each of the 12 cases.}
\label{early_coloring_method_exp1}
\end{table}
\end{center}
Results are available in Table \ref{early_coloring_method_exp1}. We see that the bigger the hidden cluster, the easier it seems to be to find which is expected as the random chance of finding it is also higher. Nevertheless for each dimension the proportion of runs in which the method works is way higher than the proportion of points that are part of the hidden cluster, so the method seems effective. Using the MAC pivot rule seems to help, but that is hard to tell for certain. On the other hand, the DGSW variant produces results that are roughly similar to the original GSW variant.

\subsection{Can we stop each run earlier ?}
As we're only interested in which element is colored the earliest on average, we could try stopping early for example between the 50th and 100th time step in order to gain time in the process.

\subsection{Can we use even less runs for our average ?}

\section{Conclusion}
%using the algorithm in machine learning to balance training and testing set, maybe balance batches in deep learning but probably too computationally expensive.
\newpage
%\appendix
%\section{This is an appendix}


\newpage
\nocite{*}
\bibliography{references}
\bibliographystyle{plain}

% Add the References to the table of contents.
\addcontentsline{toc}{section}{References}

\end{document}
