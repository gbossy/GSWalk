\documentclass[12pt]{article}
\usepackage[margin=68pt]{geometry}%48 originally, commented originally
%
\usepackage{amsthm}
\usepackage{amsmath}
%\usepackage{parskip}
\usepackage{rotating}
\usepackage{cite}
\usepackage{lmodern} % fix problems with typewriter font
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage%[demo]
{graphicx}
\usepackage{subcaption}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{enumerate}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{pgf,tikz,pgfplots}

%to include pdf plots
\usepackage{pdfpages}

%To do loops
\usepackage{multido}

%for indicator 1
\usepackage{dsfont}

%to have plots be textwidth
\usepackage{tikzscale}

%to have smaller space between figure and caption
\usepackage[font=small,skip=0pt]{caption}

%to bold stuff
\usepackage{bm}

%to include plots generated in python
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\DeclareMathOperator{\Span}{span}

%for arrowed equations
\usepackage{witharrows}
\WithArrowsOptions{tikz={font={\normalfont\small}}}


\DeclareMathAlphabet{\mymathbb}{U}{BOONDOX-ds}{m}{n}
\usepackage[linesnumbered,boxruled]{algorithm2e}%ruled in 1st param
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage%[compact]
{titlesec}
  %  \titlespacing{\section}{0pt}{2ex}{1.ex}
  %  \titlespacing{\subsection}{0pt}{1.5ex}{1ex}
   % \titlespacing{\subsubsection}{0pt}{1ex}{1ex}
%\usepackage[usenames]{color}
    \setlength{\parskip}{0.23cm}
    \setlength{\parindent}{1em}
    
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\dom}{$dom$}
\title{Experimenting with the Gram-Schmidt Walk}
\author{Gaëtan Bossy\\
Supervision by Pr. Adam Marcus\\
Chair of Combinatorial Analysis\\}
\begin{document}
\maketitle
%Issue with lindisc(point rather than [0,1]^n vector
%\begin{center}\bf Abstract\end{center}
%\small TODO

%say that vectors are sampled uniformly everytime we talk about a ball

\section{Introduction}
Let $(X, \mathcal{S})$ be a finite set system, with $X = \{1, 2, \dots, d\}$ and $\mathcal{S}= \{S_1, S_2, \dots S_n\}$ a collection of subsets of $X$. Given a coloring function $x : X \rightarrow \{-1, 1\}$, the discrepancy of a set $S$ is defined as $x(S)= |\sum_{i \in S}x(i)|$. It is a measure of how balanced $S$ is with respect to the coloring $x$. The discrepancy of the set system $(X,\mathcal{S})$ is defined as 
$$disc(X,\mathcal{S}) = \min_{x: X\rightarrow \{-1, 1\}} \max_{S \in \mathcal{S}} x(S)$$
This can be seen as a specific case of a vector balancing problem. If we consider the incidence matrix $\textbf{A} \in \mathbb{R}^{d \times n}$ of the set system $(X, \mathcal{S})$, we can write 
$$ disc(\textbf{A})= \min_{\textbf{x} \in \{-1, 1\}^n} \|\textbf{A}\textbf{x}\|_\infty$$
But we could also balance vectors with coordinates different from 0 and 1, so if we have a set of vectors $\textbf{v}_1,\dots,\textbf{v}_n\in\mathbb{R}^d$ we can see its discrepancy as the minimum infinity norm of the vector of imbalances ($\textbf{Ax}$) among all colorings $\textbf{x}\in[-1,1]^n$.

In 1997, polish mathematician Wojciech Banaszczyk published \textit{Balancing vectors and Gaussian measures of $n$-dimensional convex bodies}\cite{banaszczyk1998balancing} in which we can read the following theorem:
\begin{theorem}[Banaszczyk's Theorem from \cite{banaszczyk1998balancing}]\label{banaszczyk}
For all convex body $K \subseteq \mathbb{R}^m$, with Gaussian measure $\gamma_m(K)\geq 1/2$, and given $\textbf{v}_1, \dots, \textbf{v}_n \in \mathbb{R}^m$, $\|\textbf{v}_i\|_2 \leq 1$ for all $i$, then there exists $ \textbf{x} \in \{-1, 1\}^n$ such that
$$\sum_{i=1}^n \textbf{x}(i)\textbf{v}_i \in 5 \cdot K $$
\end{theorem}
The Gaussian measure of a body is defined as $$\gamma_m(S) = \mathbb{P}[\textbf{g} \in S] = \int_{\textbf{y} \in S} \frac{1}{(2 \pi)^{m/2}} e^{-||\textbf{y}||^2/2} d\textbf{y}$$
    where $\textbf{g}$ is a standard Gaussian random vector in $\mathbb{R}^m$, i.e. $\textbf{g} \sim \mathcal{N}(0, I_m)$. 

The proof of this theorem was non-constructive though, but nearly 20 years later, Dadush, Garg, Lovett and Nikolov showed that in order to get a constructive algorithm, all that is needed is a subgaussian distribution sampler. 

\begin{theorem}[\cite{construct}]\label{equivalence}
Banaszczyk's Theorem (up to the constant) is equivalent to the following statement:\\
Let $\textbf{v}_1, \dots, \textbf{v}_n \in \mathbb{R}^m$ of $\ell_2$ norm at most 1, with $\textbf{B}= (\textbf{v}_1, \dots, \textbf{v}_n) \in \mathbb{R}^{m \times n}$ and $K \subseteq \mathbb{R}^m$ a convex body  of Gaussian measure at least 1/2.
Then, there exists $\textbf{x}_0 \in [-1,1]^n$ and there exists a distribution $D$ on $\{-1,1\}^n$, such that: 
\begin{enumerate}
    \item  $\textbf{B}\textbf{x}_0 = \sum_{i=1}^n \textbf{x}_0(i)\textbf{v}_i \in K$\\
    \item  for $\textbf{x} \sim D$, $\textbf{B}(\textbf{x}-\textbf{x}_0)= \sum_{i=1}^n (\textbf{x}(i)-\textbf{x}_0(i))\textbf{v}_i$ is $\sigma$-subgaussian, for some $\sigma >0$ and if $\textbf{x}_0(i) \in \{-1, 1\}$, $\mathbb{P}[\textbf{x}(i)=\textbf{x}_0(i)]=1, \forall i$.
    \item $\mathbb{P}_{\textbf{x} \sim D}[\sum_{i=1}^n (\textbf{x}(i)- \textbf{x}_0(i))\textbf{v}_i \in c(\sigma)K] \geq 1/2$ for some $c(\sigma)$.
\end{enumerate}
\end{theorem}
In this last theorem, one can just pick $\textbf{x}_0=\textbf{0}$ because the gaussian measure bound forces it to be inside $K$, but finding the subgaussian distribution is far from trivial. The proof uses the Minimax theorem from Von Neumann \cite{neumann1928theorie}, turning the choice of the distribution in a game, and a result from Talagrand \cite{talagrand2005generic} on the norm of subgaussian vectors. In addition to all that, there is a complicated recentering procedure to show that the result still holds for non-symmetric bodies.

A couple years later, a team including three of the previous authors, Dadush, Garg and Lovett, joined by Bansal this time, actually found and analysed an algorithm to succesfully sample colorings that allows us to construct the colorings described by Banaszczyk in his work from my birth year. They call this algorithm the Gram-Schmidt walk, because it uses a procedure that can remind of Gram-Schmidt Orthogonalization. They proved that the vector of imbalances produced by the assignments of the Gram-Schmidt Walk are subgaussian with constant $\sigma^2=40$.

While the Gram-Schmidt walk was originally described as a way to get a constructive algorithm for Theorem \ref{banaszczyk}, it was later also used for experiment design by Spielman, Harshaw, Zhang and Sävje in \cite{harshaw2019balancing}. In their setting, they're looking to separate experiment units into a control group and a treatment group.  They use the balancing power of the algorithm to explicitly navigate the tradeoff between experiment unit covariate balance, that is having two groups that are similar, and robustness of the design, that is being random enough to be on average well-balanced among unobserved covariates. They improved the constant in the subgaussianity result and proved several other results which let them document the expected behavior of their design.

In this work, we will try to understand the algorithm, think about how to modify it to achieve different goals, and what other things we could use it for.

\section{Notation}
There are a few notations that will be used several times in this document:
\begin{itemize}
\item $[m]$ refers to the set $\{1,\dots,m\}$.
\item Every $\textbf{v}\in\mathbb{R}^d$ for some $d$ is seen as a column vector of $\mathbb{R}^{d\times 1}$ when it appears in a product, and for $i\in[d]$, $\textbf{v}(i)$ is the $i$th element of the vector $\textbf{v}$.
\item $\textbf{e}_k\in\mathbb{R}^{n}$ will be the vector with only 0's except for a 1 at position $k$ for some $k\in[n]$. Note that in particular for $M\in\mathbb{R}^{n\times n}$, $\textbf{e}_k^T\textbf{M}$ is the $k$th row of $\textbf{M}$ and $\textbf{Me}_k$ is its $k$th column.
\item Given a condition $c$, $\mathds{1}_{c}$ = $\begin{cases}
            1 \textrm{ if }c\textrm{ is true}\\
            0 \textrm{ if }c\textrm{ is false}
        \end{cases}$
\end{itemize}
Maybe remind pseudo inverse and properties.



\section{The Algorithm}
%Define \textbf{v}_perp A_i etc
The algorithm will take as input $\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$, and an initial coloring $\textbf{z}_0\in[-1,1]^n$. It will consist of $n$ time steps. At the end of time step $t$, we obtain a fractional coloring $\textbf{z}_t\in[-1,1]^n$. An element $i \in [n]$ is said to be \textit{alive} at time $t$ if $|\textbf{z}_{t-1}(i)|<1$, and \textit{fixed} otherwise. Let $A_t=\{i\in[n]:|\textbf{z}_{t-1}(i)|<1\}$. The \textit{pivot} $n(t)$ is an element that is alive at time $t$, which can for example be chosen randomly or as the largest indexed element, among the elements that are still alive. We define the set $V_t$ as $\Span\{\textbf{v}_i:i\in A_t,i\not=n(t)\}$. We denote by $\Pi_{V_t^\perp}$ the projection operator on $V_t^\perp$. Finally, we will need $\textbf{v}^{\perp}(t)=\Pi_{V_t^\perp}(\textbf{v}_{n(t)})$ as the projection of the pivot vector over all alive vectors. We are now ready to discover the actual pseudocode of the algorithm.

It produces an assignment $z$ such that the corresponding vector of imbalances, also referred to as the output sum of groups, has a small norm. Another way to see it is that it divides the vectors in two groups such that the sum of the vectors in each group are pretty close.

\begin{algorithm}[H]\label{walk}
{\fontsize{10}{12}
\caption{The Gram-Schmidt Walk by \cite{blues}}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$%with $\ell_2$ norm  at most 1
, an initial coloring $\textbf{z}_0\in[-1,1]^n$}
    \Output{a coloring $\textbf{z}_n \in \{-1,1\}^n$}
   $A_1=\{i\in[n]:|\textbf{z}_0(i)|<1\}$, $n(1) = \max \{i \in A_1\}$ and $t=1$.\\
    \While{$A_t\not=\emptyset$}{
        Compute $\textbf{u}_t\in\mathbb{R}^n$ such that
        $\begin{cases}
            \textbf{u}_t(n(t)) =1\\
            \textbf{u}_t(i) =0 \text{ if } i \notin A_t\\
            \textbf{v}^\perp(t) = \textbf{v}_{n(t)} + \sum_{i \in A_t\setminus\{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
        \end{cases}$\\
        $\Delta = \{\delta : \textbf{z}_{t-1} + \delta \textbf{u}_t \in [-1,1]^n\}$, let $\begin{cases}
            \delta_t^+ = \max \Delta\\
            \delta_t^- = \min \Delta
        \end{cases}$
         then $\delta_t = \begin{cases}
            \delta_t^+ \text{ w.p. } \frac{-\delta_t^-}{(\delta_t^+ - \delta_t^-)}\\
            \delta_t^- \text{ w.p. } \frac{\delta_t^+}{(\delta_t^+ - \delta_t^-)}
        \end{cases}$\\
        $\textbf{z}_t = \textbf{z}_{t-1} + \delta_t \textbf{u}_t$, $t\leftarrow t+1$,  $A_t=\{i\in[n]:|\textbf{z}_{t-1}(i)|<1\}$, $n(t) = \max \{i \in A_t\}$.
    }
    Output $\textbf{z}_t\in\{-1,1\}^n$.
    %\caption{Gram-Schmidt walk}
    }%
    \end{algorithm}
Throughout the text, we will refer to this algorithm by its initials as the \textbf{GSW}.% the sum of outputs or vector of imbalances is $\sum_{i=1}^nz_i\textbf{v}_i$. It is supposed to be short and thus close to \textbf{0}.

On can describe the algorithm as a walk: we start at a certain coloring, and at each time step we choose a direction and a length to move, then move, and repeat until we reach a vertex of the hypercube.

%The interesting part is how to choose the direction and length of the move in a smart way to stay as balanced as possible. Defining $\textbf{v}_\perp$ as the least we will move while pushing towards coloring the pivot, it then follows that we want to push as much as possible in that direction until an element gets colored, and our $\textbf{v}_\perp$ thus changes.

\subsection{Observations}
According to the choice of $\delta_t$, it is clear that at least one element gets frozen at each time step as $\delta_t$ is maximal such that $\textbf{z}_t+\delta_t\textbf{u}_t\in[-1,1]^n$. Thus the GSW algorithm runs in at most $n$ iterations. If we chose $\delta_t$ according to some more complicated distribution, it wouldn't be possible to guarantee a coloration per time step and the algorithm would lose most of its appeal and simplicity. Additionally, the update direction depends on the elements that are alive. Thus if they did not change between two time steps, the update direction would not change either and we would just keep moving along the same line until we hit a border, which is why choosing one of the two maximal valid lengths, $\delta_+$ or $\delta_-$, is the best method to choose the length of the move.

We can see that
$$\mathbb{E}[\delta_t \mid \delta_t^-, \delta_t^+] = \delta_t^+\frac{-\delta_t^-}{\delta_t^+-\delta_t^-} + \delta_t^- \frac{\delta_t^+}{\delta_t^+-\delta_t^-} =0$$
so the choice of delta and thus the sequence of fractional coloring produced by the algorithm before its termination is a martingale. This gives a sort of unbiasedness to the algorithm which is desirable when dividing elements into groups. It also means that we can tune the final outcome through the starting coloring.

One can see that $\textbf{v}^\perp _t$ is simply the last vector of the Gram-Schmidt orthogonalization of the ordered sequence of vectors $(\textbf{v}_i)_{i\in A_t}$. This is where the name of the algorithm comes from.

It is important to notice that $\textbf{v}^{\perp}_t$ depends on $t$ and not just on $\textbf{v}_{n(t)}$, as $A_t$ can change while the pivot $n(t)$ stays the same.

The update direction $\textbf{u}_t$ satisfying our conditions always exists because
\begin{align*}
        \textbf{v}^\perp_t = \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
        \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i = -(\textbf{v}_{n(t)} - \textbf{v}^\perp_t) \in V_t
\end{align*}

The vector $\textbf{u}_t$ is defined with $\textbf{u}_t(i)=0$ if $i$ is frozen, $\textbf{u}_t(n(t))=1$ and for the rest of the indices we have that :
\begin{align*}
\textbf{v}^\perp(t) &= \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i\\
\Leftrightarrow \textbf{v}^\perp(t) &= 1 \cdot \textbf{v}_{n(t)} + \sum_{i \in A_t \setminus \{n(t)\}} \textbf{u}_t(i)\textbf{v}_i + \sum_{i \notin A_t} 0 \cdot \textbf{v}_i\\
\Leftrightarrow \textbf{v}^\perp(t) &= \sum_{i=1}^n \textbf{u}_t(i)\textbf{v}_i
\end{align*}

$\textbf{v}^\perp_t$ will correspond to the direction that the output of the random walk will move in during time step t, ie if the matrix $\textbf{B}$ contains the input vectors as columns, $\textbf{v}^\perp_t = \textbf{Bu}_t$. So the algorithm is greedily optimal in the sense that the update direction is trying to add as little discrepancy as possible while moving towards a vertex of the hypercube.

$\textbf{u}_t = \arg\min_{\textbf{u} \in U} \|\textbf{Bu}\|$ where $U$ is the set of vectors in $\mathbb{R}^n$ such that $\textbf{u}(i) = 0 , i \not\in A_t$ and $\textbf{u}(n(t))=1$. This means that one can completely forget about $\textbf{v}^\perp(t)$ and system solving and just use least square at each step to find the coordinates of the update directions that aren't alive or the pivot through the following formula $$u(A_t\setminus\{n(t)\})=\arg\min_u\|\textbf{v}_{n(t)}+\sum_{i\not\in A_t\setminus\{n(t)\}}u(i)\textbf{v}_i\|^2=(\textbf{B}_t^T\textbf{B}_t)^{-1}\textbf{B}_t^T\textbf{v}_{n(t)}$$ where $\textbf{B}_t$ is the matrix containing all vectors that are alive but not the pvot as columns. In some case, when $\textbf{v}^\perp(t)=\textbf{0}$ there are infinitely many solutions that minimize our objective function, so it would be interesting to add some conditions or do some quadratic programming to try to get a $\textbf{u}_t$ that has some desirable properties.

Another interesting point is that one can choose each new pivot at random among alive elements. This is equivalent to shuffling the input vectors before inputing them to the walk, and does not matter when doing a single run. One thing that can be interesting though, is seeing how the choice of the pivot affects the behavior of the algorithm, as the choice of the pivot doesn't need to follow a specific rule except for one small result about confidence intervals from \cite{harshaw2019balancing}. Thus, trying to optimize the algorithm for our needs by tweaking the choice of the pivot could be an interesting direction to explore.

\subsection{Basic Variants}\label{basic_variants}
One pretty close variant would always choose the delta with the smallest absolute value instead of $\delta_t$ being a martingale, which we will denote as the Deterministic Gram-Schmidt Walk, or DGSW. In case of equality between the two absolute value, the algorithm would still pick randomly. The results from section \ref{results} do not apply to this variant, but in practice it is pretty close to the classical GSW, except it is expectedly returning slightly shorter vector of imbalances. Notice that there is still randomness in the order of the pivots, which can be done either by shuffling the vectors before the run or by choosing the pivot randomly among alive elements when needed.

Another really different variant which we will compare a bit with the GSW in this document is the following algorithm:

\begin{algorithm}[H]\label{naivewalk}
{\fontsize{10}{12}
\caption{The Naive Walk}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$%with $\ell_2$ norm  at most 1
}
    \Output{a coloring $\textbf{a} \in \{-1,1\}^n$}
   $\textbf{s}=\textbf{0}\in\mathbb{R}^d$\\
   $\textbf{a}=\textbf{0}\in\mathbb{R}^n$\\
    \For{$i\in [n]$}{
      \eIf{$\|\textbf{s}+\textbf{v}_i\|<\|\textbf{s}-\textbf{v}_i\|$}{$\textbf{a}(i)=1$; $\textbf{s}=\textbf{s}+\textbf{v}_i$}{$\textbf{a}(i)=-1$; $\textbf{s}=\textbf{s}-\textbf{v}_i$}
    }
    Output $\textbf{a}\in\{-1,1\}^n$.
    }%
    \end{algorithm}
This simple algorithm that we call the Naive Walk can have a random component through a pre-input shuffling of the vectors, similarly to the DGSW. It doesn' actually perform badly in term of norm of the resulting vectors of imbalances, as can be seen via the experiment in section \ref{how_good_at_minimizing_disc}.

Another way to do it would be to choose the sign according to a distribution that is inversely proportional to the norm of the potential updated $\textbf{s}$ vector, or to greedily try every assignment of every vector at each step and add the assignment that results in the smallest norm, or do a distribution based on how big the norm would be if an assignment was added. In the end, I wanted to compare with a simple algorithm and chose this one.

%\subsection{Examples}
%One in 3d, then the graph with all kind of different outputs on it
\begin{figure}[h]
\include{4types_3}
\caption{Plot of ouput sum of group assignments from the GSW, its input vectors, and a centered normal with $\textbf{Cov}=\textbf{I}_2$. There are 100 input vectors and 100 of each kind of point, everything is in dimension 2. The same plot with vector of imbalances from random assignment is visible in Figure \ref{4types_4} for comparison.}
\label{4types_3}
\end{figure}
\begin{figure}[h]
\include{4types_4}
\caption{Plot of ouput sum of group assignments from the GSW, its input vectors, and a centered normal with $\textbf{Cov}=\textbf{I}_2$ similarly to Figure \ref{4types_3}, except we added the output sums of random group assignments for comparison. There are 100 input vectors and 100 of each kind of point, everything is in dimension 2.}
\label{4types_4}
\end{figure}

\begin{figure}
\begin{center}
\newpage
\multido{\i=0+1}{3}{
\includegraphics[width=12.7cm]{3d_example/gswalkboth\i.pdf}
}
\caption{Example of a GSW run in with 3 vectors in 2 dimension. The left part shows the cube where the coloring is living, and the right part shows the output sum and the input vectors.}
\label{3d_example}
\end{center}
\end{figure}


\subsection{Results}\label{results}
\begin{definition}
A random vector $\textbf{Y} \in \mathbb{R}^m$ is said to be subgaussian with parameter $\sigma$ (or $\sigma$-subgaussian) if for all $\bm{\theta} \in \mathbb{R}^m$:
$$\mathbb{E}[e^{\langle\textbf{Y},\bm{\theta}\rangle}]\leq e^{(\sigma^2/2)\|\bm{\theta}\|_2^2}$$ \\
\end{definition}
This definition will help us give a concrete result on the balancing power of the GSW. A random vector being $1$-subaussian means that it is less spread that a gaussian with covariance matrix being the identity. We can see that for the output sum of the GSW, on Figure \ref{4types_3}, it seems to indeed be the case. This was indeed proved, first for $\sigma^2=40$ in \cite{blues} and then for $\sigma^2=1$ in \cite{harshaw2019balancing}.
\begin{theorem}[\cite{harshaw2019balancing}]
    For $\textbf{z}$ sampled via the Gram-Schmidt walk design, we have that $\textbf{Bz}$ is subgaussian with parameter $\sigma^2=1$:$$\mathbb{E}[exp(\langle\textbf{Bz},\textbf{v}\rangle)]\leq exp(\|\textbf{v}\|^2_2/2)\quad\forall\textbf{v}\in\mathbb{R}^{n+d}$$
\end{theorem}
This result is necessary to prove that the GSW indeed gives a constructive algorithm for Theorem \ref{banaszczyk}, by using Theorem \ref{equivalence}. 
%\begin{definition}
%We say that $L\in\mathbb{R}^{n\times n}$ is bounded in the Loewner order by $M\in\mathbb{R}^{n\times n}$, also written as $L\preceq M$ if $M-L$ is positive semi-definite, that is $z^T(M-L)z\geq 0$ $\forall z\in\mathbb{R}^n$.
%\end{definition}
%This other result from \cite{harshaw2019balancing} gives us a bound on the covariance of the vector of imbalances, which tells us that TODO
%\begin{theorem}[\cite{harshaw2019balancing}]
%If all input vectors of the GSW $\textbf{v}_1,\dots,\textbf{v}_n$ have $\ell_2$ norm at most 1, then the covariance
%matrix of the vector of imbalances $Bz$ is bounded in the Loewner order by the orthogonal projection onto the subspace spanned by the columns of $B$: $$Cov(Bz)\preceq P = B(B^TB)^\dagger B^T,$$
%where we recall that $M^\dagger$ is the pseudoinverse of the matrix $M$.
%\end{theorem}
\section{Fast computations}\label{fast_computations}
Thanks to an original idea from professor Marcus, one can fasten the GSW from the trivial implementation which takes time $O(n^3d)$ down to $O(n^2d)$, similarly to what was done in \cite{harshaw2019balancing} using Cholesky factorizations, except the proofs are easier. The basic idea is similar: at the beginning of a GSW run, we compute some matrices and then during each step of the algorithm, we use only rank one updates to maintain them and derive the update direction from them at each iteration. This results in the removal of an $n$ factor from the asymptotic running time since rank-one update can be computed in quadratic time rather than the cuic time necessary to compute matrix multiplications and inverses.

For some set $S\subset\{1,\dots,n\}$, let $\textbf{I}_S\in\mathbb{R}^{n\times n}$ be the diagonal $n\times n$ matrix with $\textbf{I}_S(i,i) =1$ if $i\in S$ and $0$ otherwise, and $\textbf{B}_S\in\mathbb{R}^{d\times n}$ be $\textbf{BI}_S$ where $\textbf{B}$ is as previously the matrix containing the input vectors as columns. We define $\textbf{C}_S\in\mathbb{R}^{n\times d}$ to be such that $\textbf{C}_S\textbf{B}_S=\textbf{I}_S$ and if $i\not\in S$, the $i$th row of $\textbf{C}_S$ is $\textbf{0}$. That is, each row of $\textbf{C}_S$ whose index is in $S$ is the pseudo-inverse of the corresponding column in $\textbf{B}_S$. Finally, $\textbf{D}_S\in\mathbb{R}^{n\times n}$ is $\textbf{C}_S\textbf{C}_S^T$.

This technique necessitates that the dimension of the space spanned by the input vectors is at most the dimension $d$ of the input vectors, as otherwise $\textbf{C}_S$ doesn't exist. %For the other case, one could produce another technique.

Here are the rank one update that let us compute ou matrix for some $S'=S\setminus\{k\}$.
\begin{lemma}\label{update_lemma}
For $k\in S$, let 
\begin{itemize}
\item $\textbf{c}_k\in\mathbb{R}^{n}$ be the $k$th row of $\textbf{C}_S$
\item $\textbf{d}_k\in\mathbb{R}^{d}$ be the $k$th row of $\textbf{D}_S$
\item $S'= S\setminus\{k\}$
\end{itemize}
Then $\textbf{C}_{S'}=\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T$ and $\textbf{D}_{S'}=\textbf{D}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{d}_k^T$.
\end{lemma}
\begin{proof}
For the first part, we have that\begin{align*}
\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)\textbf{B}_{S'}&=\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)\textbf{B}_{S}\textbf{I}_{S'}\\
&=\textbf{C}_S\textbf{B}_S\textbf{I}_{S'}-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\textbf{B}_{S}\textbf{I}_{S'}\\
&=\textbf{I}_{S'}-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{e}_k^T\textbf{I}_{S'}\\
&=\textbf{I}_{S'}-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{0}\\
&=\textbf{I}_{S'}\end{align*}
Thus we just need to show that the rows of $\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T$ not in $S'$ are 0. As $\frac{1}{\textbf{d}_k(k)}\textbf{d}_k(k)=1$, we have that the $k$th row of $\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T$ is indeed 0. Lastly, as $\textbf{D}_S=\textbf{C}_S\textbf{C}_S^T$, we have that $i\not\in S\Rightarrow \textbf{d}_k(i)=0$ thus all the rows whose indices aren't in $S$ indeed do stay 0.

For the second part, we can see that \begin{align*}
\textbf{D}_{S'}&=\textbf{C}_{S'}\textbf{C}_{S'}^T\\
&=\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)^T\\
&=\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)\left(\textbf{C}_S^T-\frac{1}{\textbf{d}_k(k)}\textbf{c}_k\textbf{d}_k^T\right)\\
&=\textbf{C}_S\textbf{C}_S^T-\frac{1}{\textbf{d}_k(k)}\textbf{C}_S\textbf{c}_k\textbf{d}_k^T-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\textbf{C}_S^T+\frac{1}{\textbf{d}_k(k)^2}\textbf{d}_k\textbf{c}_k^T\textbf{c}_k\textbf{d}_k^T\\
&=\textbf{D}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{d}_k^T-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{d}_k^T+\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{d}_k^T\\
&=\textbf{D}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{d}_k^T\end{align*}\end{proof}
Now let's see why this is useful, that is, how to compute $\textbf{v}^\perp(t)$ and $\textbf{u}_t$ from these matrices.
\begin{lemma}
Assume we're currently in a step of the GSW where the active vectors are $A_t=\{\textbf{v}_s:s\in S\}$, and $\textbf{v}_k\in A_t$ is the pivot. If $\textbf{d}_k$ and $\textbf{c}_k$ are defined similarly as in Lemma \ref{update_lemma}, we have that $$\textbf{v}^\perp(t)=\frac{\textbf{c}_k}{\|\textbf{c}_k\|^2}\textrm{ and }\textbf{u}_t=\frac{\textbf{d}_k}{\textbf{d}_k(k)}.$$
\end{lemma}
\begin{proof}
Let $S'=S\setminus \{k\}$. Starting from the definition of $\textbf{v}^\perp(t)$ with pivot $k$, we have that \begin{equation*}\begin{WithArrows}
\textbf{v}^\perp(t)&=\Pi_{V_t^\perp}(\textbf{v}_k)\Arrow{$\Span V_t\cup V_t^\perp=\mathbb{R}^d$}\\
&=\textbf{v}_k-\Pi_{V_t}(\textbf{v}_k)\Arrow{Definition of $\Pi_{V_t^\perp}$ with pivot $k$}\\
&=\textbf{v}_k-\textbf{B}_{S'}\textbf{C}_{S'}\textbf{v}_k\Arrow{Lemma \ref{update_lemma}}\\
&=\textbf{v}_k-\textbf{B}_{S'}\left(\textbf{C}_S-\frac{1}{\textbf{d}_k(k)}\textbf{d}_k\textbf{c}_k^T\right)\textbf{v}_k\Arrow{$\textbf{C}_i\textbf{v}_k^T=\mathds{1}_{i=k}$}\\
&=\textbf{v}_k-\textbf{B}_{S'}\textbf{e}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S'}\textbf{d}_k\textbf{c}_k^T\textbf{v}_k\Arrow{$k$th line of $\textbf{B}_{S'}$ is \textbf{0} as $k\not\in S'$}\\
&=\textbf{v}_k+\textbf{0}+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S'}\textbf{d}_k\Arrow{$\textbf{d}_k$ is the $k$th row of $\textbf{D}_S=\textbf{C}_S\textbf{C}_S^T$}\\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S'}(\textbf{e}_k^T\textbf{C}_S\textbf{C}_S^T)^T\Arrow{$(\textbf{AB})^T=\textbf{B}^T\textbf{A}^T$}\\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S'}\textbf{C}_S\textbf{C}_S^T\textbf{e}_k\\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S'}\textbf{C}_S\textbf{c}_k\Arrow{Definition of $\textbf{B}_{S'}$} \\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\left(\textbf{B}_{S}-\textbf{v}_k\textbf{e}_k^T\right)\textbf{C}_S\textbf{c}_k\\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S}\textbf{C}_S\textbf{c}_k^T-\frac{1}{\textbf{d}_k(k)}\textbf{v}_k\textbf{c}_k^T\textbf{c}_k\Arrow{$\textbf{D}_S=\textbf{C}_S\textbf{C}_S^T$}\\
&=\textbf{v}_k+\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S}\textbf{C}_S\textbf{c}_k^T-\frac{\textbf{d}_k(k)}{\textbf{d}_k(k)}\textbf{v}_k\\
&=\frac{1}{\textbf{d}_k(k)}\textbf{B}_{S}\textbf{C}_S\textbf{c}_k^T\Arrow{Pseudo-inverse properties}\\
&=\frac{1}{\textbf{d}_k(k)}(\textbf{B}_{S}\textbf{C}_S)^T\textbf{c}_k^T\Arrow{$(AB)^T=B^TA^T$}\\
&=\frac{1}{\textbf{d}_k(k)}\textbf{C}_S^T\textbf{B}_{S}^T\textbf{c}_k^T\Arrow{Pseudo-inverse properties}\\
&=\frac{1}{\textbf{d}_k(k)}\textbf{c}_k^T\Arrow{$\textbf{D}_S=\textbf{C}_S\textbf{C}_S^T$}\\
&=\frac{1}{\|\textbf{c}_k\|^2}\textbf{c}_k^T
\end{WithArrows}\end{equation*}

For the second part, we have that $\textbf{u}_t$ is the solution to the system \begin{align*}\textbf{u}_t(k) =1\\
            \textbf{u}_t(i) =0 \text{ if } i \notin A_t\\
            \textbf{v}^\perp(t) = \textbf{v}_{k} + \sum_{i \in A_t\setminus\{n(t)\}} \textbf{u}_t(i)\textbf{v}_i=\textbf{B}_Su_t\end{align*}
But we have that $$\textbf{v}^\perp(t)=\frac{\textbf{c}_k}{\|\textbf{c}_k\|^2}=\frac{\textbf{C}_S^T\textbf{e}_k}{\|\textbf{c}_k\|^2}$$
if we multiply by $\textbf{C}_S$ we get $\frac{\textbf{C}_S\textbf{C}_S^T\textbf{e}_k}{\|\textbf{c}_k\|^2}=\textbf{u}_t$ But as $\textbf{C}_S\textbf{C}_S^t=\textbf{D}_S$ and $\|\textbf{c}_k\|^2=\langle \textbf{c}_k,\textbf{c}_k\rangle=\textbf{d}_k(k)$, we have that indeed $\frac{\textbf{d}_k}{\textbf{d}_k(k)}=\textbf{u}_t$.
\end{proof}
These lemmas mean that once we have computed $\textbf{C}_S$ and $\textbf{D}_S$ at the beginning of the GSW in time $O(n^2d+n^3)$, we can update them in time $O(nd+n^2)$ and store them with a similar quantity of memory, resulting in a total time of running of $O(n^2(d+n))$ as computing $\delta_t$ and updating the coloring takes $O(n)$ time.

This happens to not be much faster in practice than a clever implementation using rank-one update to compute the inverse necessary to compute the Least Squares solution. That is because in the latter case, the size of the inverse matrix decreases at each step of the algorithm in that case, thus the one matrix multiplication necessary to compute the update direction becomes faster than the rank one updates described in this section quickly when $n$ is small, as the $\textbf{C}_S$ and $\textbf{D}_S$ keep the same size throughout the whole algorithm run.


\section{Generalizing to any hyperparallelepiped}
The idea is to allow the algorithm to sample coloring not only on the hypercube $[-1,1]^n$, but also on any set $\textbf{b}_1,\dots, \textbf{b}_n\in\mathbb{R}^n$ spanned by $n$ linearly independent vectors, denoted as the basis $\mathcal{B}$. It then won't be a coloring per se but just a linear combination of the vectors with some coefficients corresponding to a vertex of the hyperparallelepiped formed by the basis vectors. We will still denote it by the term coloring for clarity reasons.

To do so, we have to adapt the algorithm at several points. Firstly, an element $k$ associated to the basis vector $\textbf{b}_k$ should be alive only if the current coloring is not on one of the 2 facets such that when expressed in the basis $\textbf{b}_1,\dots,\textbf{b}_n$, the $k$th coordinate is -1 or 1. 

Secondly, choosing the update direction is then different as we need to stay orthogonal from every fixed vector, but these vectors don't correspond to coordinates anymore.

Thirdly, the computation of the two potential $\delta$ is different as well for some similar reasons: you have to compute the maximum delta to sty in the hyperparallelepiped defined by the basis

All these issues can be solved via some basis changes in the right spots and the modified algorithm is very similar, and should work exactly the same way when given the orthonormal canonical basis of $\mathbb{R}^n$, $\textbf{e}_1,\dots,\textbf{e}_n$. The main idea is that we will keep two separate coloring, $\textbf{z}$ and $\textbf{z}'$, where $\textbf{z}$ is expressed in the canonical basis and $\textbf{z}'$ is expressed as coordinates in $\mathcal{B}$. We will update $\textbf{z}'$ with $\textbf{u}_t'$, that is $\textbf{u}_t$ except it is expressed via its coordinates in $\mathcal{B}$. Computing $\delta_t$ is then fairly easy as we just have to replace $\textbf{z}$ by $\textbf{z}'$. 

Let $\textbf{B}_{\mathcal{B}}$ be the matrix containing the basis vectors $\textbf{b}_1,\dots,\textbf{b}_n$ as columns. In order to compute $\textbf{u}_t'$, we replace the matrix containing our input vectors as columns, $\textbf{B}$, by $\textbf{B}\textbf{B}_{\mathcal{B}}$. We then apply the same method as usual to find $\textbf{u}_t'$, and change its basis to get $\textbf{u}_t$.

Putting it all in pseudocode and highlighting the difference with the original algorithm (\ref{walk}) in \textcolor{red}{red}, we get the Algorithm \ref{basiswalk}.\\
\begin{algorithm}[H]\label{basiswalk}
{\fontsize{10}{12}
\caption{The Gram-Schmidt Walk on a different basis}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{$\textbf{v}_1,\ldots,\textbf{v}_n\in\mathbb{R}^d$%with $\ell_2$ norm  at most 1
, an initial coloring $\textbf{z}_0\in[-1,1]^n$,   \textcolor{red}{$n$ linearly independent basis vectors $b_1,\dots,b_n\in\mathbb{R}^n$}}
    \Output{a coloring $\textbf{z}_n \in \textcolor{red}{\mathbb{R}^n}$}
   \textcolor{red}{$z_0'=\textbf{B}_{\mathcal{B}}^{-1}z_0$}\\
   $A_1=\{i\in[n]:|\textbf{z}_0'(i)|<1\}$, $n(1) = \max \{i \in A_1\}$ and $t=1$.\\
    \textcolor{red}{$B=(\textbf{v}_1,\dots,\textbf{v}_n)\textbf{B}_{\mathcal{B}}$}\\
    \While{$A_t\not=\emptyset$}{
        Compute $\textcolor{red}{\textbf{u}_t'}\in\mathbb{R}^n$ such that
        $\begin{cases}
              \textcolor{red}{\textbf{u}_t'}(n(t)) =1\\
              \textcolor{red}{\textbf{u}_t'}(i) =0 \text{ if } i \notin A_t\\
            \textbf{v}^\perp(t) =   \textcolor{red}{B\textbf{u}_t'}\\
        \end{cases}$\\
        $\Delta = \{\delta : \textcolor{red}{\textbf{z}_{t-1}' + \delta \textbf{u}_t'} \in [-1,1]^n\}$, let $\begin{cases}
            \delta_t^+ = \max \Delta\\
            \delta_t^- = \min \Delta
        \end{cases}$
         then $\delta_t = \begin{cases}
            \delta_t^+ \text{ w.p. } \frac{-\delta_t^-}{(\delta_t^+ - \delta_t^-)}\\
            \delta_t^- \text{ w.p. } \frac{\delta_t^+}{(\delta_t^+ - \delta_t^-)}
        \end{cases}$\\
        $  \textcolor{red}{\textbf{z}_t' = \textbf{z}_{t-1}' + \delta_t \textbf{u}_t'}$, $t\leftarrow t+1$,  $A_t=\{i\in[n]:|  \textcolor{red}{\textbf{z}_{t-1}'}(i)|<1\}$, $n(t) = \max \{i \in A_t\}$.
    }
    Output $  \textcolor{red}{\textbf{B}_{\mathcal{B}}\textbf{z}_t'\in\mathbb{R}^n}$.
    %\caption{Gram-Schmidt walk}
    }%
    \end{algorithm}
An example of output of the GSW with a different coloring basis can be found in Figure \ref{3types_basis_200_2}, that contains a plot of vectors of imbalances of the classical GSW, and the GSW with two different basis; one that is orthonormal and one that is all positive vectors of norm 1. We see that the vectors of imbalances of the former resemble closely those of the classical GSW, while the latter has a skewed distribution. 
\begin{figure}[h]
\include{3types_basis_200_2}
\caption{Plot of vectors of imbalances of group assignments from the GSW, the GSW with a basis where every vector has only positive coordinates and is of norm 1 (\textbf{B1}) and the GSW with a random orthonormal basis (\textbf{B2}). There are 200 input vectors from $\mathbb{R}^2$, and 200 of each type of output. }\label{3_types_basis}
\end{figure}

\section{Generalizing to more than two groups}
Discrepancy minimization is generally set in a 2-group paradigm, but it could be interesting to generalize the GSW to separate into more than 2 groups. For example, if one wanted to separate $n$ vectors in 3 groups, the GSW could first be used to separate into groups $G_+,G_-$ such that $\sum_{\textbf{v}\in G_+}\textbf{v}-2\cdot\sum_{\textbf{v}\in G_-}\textbf{v}\approx \textbf{0}$, by starting at $\textbf{z}_0=(1/3,1/3,\dots,1/3)\in\mathbb{R}^n$. Then the group $G_+$ could be again inputed into the GSW to separate it into $G_{++}$ and $G_{+-}$, and we would expect $G_-,G_{++}$ and $G_{+-}$ to be roughly balanced mutually but also all together.

But how do we know if a 3 group assignment $G_i$ for $i\in\{0,1,2\}$ is balanced ? For 3, one could use the complex roots of 1, $\omega_0=1 ,\omega_1$, and $\omega_2$, and check that $$\sum_{i\in\{0,1,2\}}\omega_i\sum_{\textbf{v}\in G_i}\textbf{v}\approx\textbf{0}.$$

One issue it that this seems like a bandaid method, and it seems like it would yield before grou assignment to have an algorithm that separates in $m$ groups from the get go.

Another issue is that this doesn't generalize to a higher number of groups, which is why seeing we need another perspective. One  idea is to link each group with a vertex of the $(m-1)$-dimensional regular simplex centered in \textbf{0} where $m$ is the number of groups we want to separate our vectors into. So we would want to assign each group to a vertex and verify that the sum of our vectors is close to \textbf{0} in each of the $m-1$ dimensions. This would mean that our coloring would live in $S_{m-1}^n$, each vector moving in its personal copy of the simplex until it gets fixed to one of the vertices.

We would have to adapt the choice of the update direction and the choice of $\delta$ which could maybe also be multi-dimensional. One big issue then is that choosing an update direction is far from obvious. Should we force the multidimensional vector of update of the pivot to be of norm 1 ? If so how to choose it ? Additionally, assuming we have an update direction, what should one do when the border of the simplex is hit but not the vertex ? Should we now force the coloring to stay fixed to that border and now move in that border ? Should we choose the update direction and $\delta$ in a way that borders are never hit outside of vertices ?

All these questions are tough to answer, and generalizing the GSW to separate in $m$ groups would require understanding them deeply. Sadly, I did not succeed in finding such a generalization.

\section{Experiments and Properties}
In this section, we try to see how the GSW behaves, both generally and when given some specific inputs. We also try some modifications that aim to modify its behavior for some specific purposes.

\subsection{How good is the GSW at minimizing output discrepancy in practice ?}
We're interested in seeing how well does the GSW actually perform in minimizing the norm. We will compare it to the naive walk (Algorithm \ref{naivewalk}), the deterministic GSW (also referred to as DGSW, defined in section \ref{basic_variants} and the actual lowest discrepancy assignment computed via bruteforcing for small $n$s.

\subsubsection{Experiment}\label{how_good_at_minimizing_disc}
We compare the output discrepancy of GSW, DGSW, the naive walk, \textbf{NW}, and for some small $n$ also the best assignment found via brute forcing on all possibilities, \textbf{LDA}. We do this for $n=5,10,20,40,80,160$, and with $d=2^i$ for $i\in\{1,\dots,15\}$, where we sample $n$ vectors from the $d$-dimensional ball of radius 1. We do one thousand runs for each $d$ and each $n$, each with a different set of $n$ vectors sampled from the ball of radius 1.

\subsubsection{Results}
\begin{figure}
\centering
\include{comparative_norms_n=160_repeat=1000_max_dim=32768.0}
\caption{Result of the experiment from section \ref{how_good_at_minimizing_disc}: comparison of different discrepancy minimizing vectors for $n=5,10,20,40,80$ and $160$ vectors of dimension up to $2^{15}$. Results were averaged over 1000 runs.}\label{output_disc}
\end{figure}
Our results are visible in Figure \ref{output_disc}. We can see that GSW actually gives the worst results in terms of discrepancy minimization, but that when the dimension of the vector grows all methods seem to give similar results asymptotically. Note that we cannot say that the naive walk is just a better discrepancy-minimizing algorithm as these results would probably be different if we modified the distribution of input vectors.

\subsection{Does translation affect the balance of the assignment ?}\label{trans_balance}
By \textit{balance of the assignment}, we mean the absolute difference between the number of 1s and -1s. An assignment is perfectly balanced if its sum is 0, that is the groups produced by the assignment have equal size.

If our initial group of vector is centered around \textbf{0}, we would expect that translating it away will force it to have a greater balance between -1s and 1s in order to balance the translation part added to each vector.

\subsubsection{Experiment}
We sample 200 input vectors from the ball in dimension 200. We then run the GSW and DGSW 100 times each on those vector translated by some random norm 1 vector multiplied by some factor. We use the factors 0,1,2,5 and 10 and compare the results.

\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 Factor & 0 & 1  & 2 & 5 & 10  \\
\hline
GSW  & 9.7 & 0.96 & 0.46 & 0.1 & 0.06 \\
DGSW & 44.5 & 1.78 & 0.32 & 0.08 & 0.04
\end{tabular}
\caption{Results of our experiment on the balance of assignments depending on how much the vectors are transated. The numbers shown are the average absolute value of the sums of output vectors. A smaller number indicates that the assignment has a more balanced assignment, that is the number of 1s and -1s are closer.}\label{balance_when_translated}\end{table}\end{center}
We can see that indeed the further away from 0 our input vectors are translated the more balanced the assignments are, as expected. It's interesting to notice that assignments from the DGSW are way less balanced than with the GSW. I currently do not have any hypothesis on why that is.

These experiments make us want to try to build a variant of GSW that can have a balance parameter thanks to the balancing properties of translation. That is what we try in the following experiment.

\subsection{A parameter to balance assignments}\label{balance_parameter}
Inspired by section \ref{trans_balance}, we propose a slight modification of the GSW that pushes toward balanced assignments. The idea is to add a coordinate to the input vector and give more or less importance to that coordinate similarly to how the balance-robustness tradeoff is implemented in \cite{harshaw2019balancing}, except here we implement a tradeoff between assignment balance and norm of the vector of imbalances.

Given input vectors $\textbf{v}_1,\dots,\textbf{v}_n\in\mathbb{R}^d$ and a parameter $\mu\in[0,1]$, we define $\textbf{w}_1,\dots,\textbf{w}_n\in\mathbb{R}^{d+1}$ as $$\textbf{w}_i=\begin{pmatrix}\sqrt{1-\mu}\textbf{v}_i \\ \sqrt{\mu}\end{pmatrix}.$$ This way the $\textbf{w}_i$'s have similar norm to the $\textbf{v}_i$'s, but it is possible that a different normalization depending on the norm of the $\textbf{v}_i$'s could be better. We then run the GSW on them and use the output assignment on the original vectors. Choosing $\mu=0$ is equivalent to doing the classical GSW algorithm, while using $\mu=1$ is equal to forcing exact balance. We run experiments to determine how much balance in the assignment we gain and how much further from \textbf{0} our output is for different $\mu$s.

\subsubsection{Experiment}
We use our just explained construction to compute an assignment on the $w_i$'s using GSW or DGSW, then see how this assignment performs on the original $\textbf{v}_i$'s in terms of balance of the assignment and discrepancy. We also program the fixed size GSW described in \cite{harshaw2019balancing} and compare it. We use $\mu\in\{0,0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.999,1\}$, and $n$ vectors sampled from the ball of radius 1 in dimension $n$ for $n=100$. The results presented are averaged over 1000 runs.

\subsubsection{Results}

\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllllllllll}
 $\mu$ &0&0.001&0.01&0.1&0.25&0.5&0.75&0.9&0.99&0.999&1&FSD \cite{harshaw2019balancing}\\
\hline
AB&8.086&6.248&4.164&1.898&1.108&0.576&0.266&0.106&0.01&0.002&0&0 \\
AD&5.259&5.275&5.266&5.311&5.341&5.321&5.336&5.331&5.334&5.337&9.917&5.317\\
ABD&20.584&18.698&11.908&3.886&2.044&0.994&0.328&0.116&0.018&0.002&0&0\\
ADD&4.863&4.837&4.775&4.821&4.885&4.938&4.991&5.004&5.008&5.002&9.851&5.01
\end{tabular}
\caption{Results of our experiment on the balance of assignments with our new balance-discrepancy tradeoff design. The balance numbers shown (lines starting with AB for average balance) are the average absolute value of the sums of output vectors, and the discrepancy numbers shown (lines starting with AD for average discrepancy) are the norms of the sum of $\textbf{v}_i\cdot x_i$, $x$ being the assignment produced by GSW for the $w_i$'s except for the last column in which we used the fixed size GSW design from \cite{harshaw2019balancing}. The D at the end of the last two lines indicates that in this case, the deterministic GSW algorithms was used. \\A smaller balance number indicates that the assignment has a more balanced assignment, that is the number of 1s and -1s are closer. A smaller discrepancy number indicates that the vectors are better balanced among the groups, that is the sum of each coordinate is closer to be the same in each group. FSD \cite{harshaw2019balancing} references the fixed size design from \cite{harshaw2019balancing}.}
\label{balance_tradeoff_results}
\end{table}
\end{center}

We can see that we can massively increase assignment balance without making the output norm much larger. For $\mu=0.999$ for example, it is very likely that an assignment is exactly balanced and thus this variant of the algorithm can provide balanced GSW assignments with high probability while keeping all properties of the classical GSW, as opposed to the balanced variant described in \cite{harshaw2019balancing}, for which we don't know if some of the original properties still hold. %The high probability comes from the subgaussianity bound.

\subsection{What else can we control by adding coordinates ?}
We could think of a design where we want several subgroups, potentially of only 2 vectors per subgroup, to be balanced. We could then add a coordinate for each subgroup and put the same number on that coordinate for each member of the subgroup, while putting 0 for every vector that isn't in the subgroup. This would push towards balancing within any subgroup we want, and could be done via some parametering similar to what was done in section \ref{balance_parameter}. 

Similarly, if we want 2 vectors to be in the same group because of some previous knowledge about the input vector set, we could add a dimension and assign some number $x$ and $-x$ to these vectors in that new dimension while giving 0 to every other vector in that dimension.

Note that if we want to force two vectors $\textbf{v}_i,\textbf{v}_j$ to be in the same or the opposite group, we could remove them from the input and add their sum or difference respectively. Then, looking at what group was assigned to the sum or difference we could get an assignment for the actual input vectors (including $\textbf{v}_i$ and $\textbf{v}_j$).

%Adding dimensions could be used to translate knowledge about the vector set into usable information for the algorithm.
Depending on the type of constraints we have on our assignment, or if we want to modify the distribution or the property of an assignment, there is a chance that this can be done by adding coordinates to the input vectors. Thus, for specific applications where the GSW is used, this technique could be useful.



%\subsection{Does norm affect the balance of the assignment ?}
%We would expect norms not to affect the balance of the assignment, as multiplying every input vector by the same factor should mean the algorithm runs similarly.

\subsection{Does norm affect when a vector is colored ?}\label{norm_affect_when}
The expected heuristic would have been that bigger vectors are colored earlier and the algorithm then colors the smaller ones to minimize discrepancy as that is what we'd instinctively do. It turns out that the algorithm actually does the reverse and colors the smaller vectors earlier than the bigger ones.

We performed two experiments to observe this behavior. In both experiments, we have vectors $\textbf{v}_1,\dots,\textbf{v}_{200}$ of increasing norm and observe how close the coloring order is to $\mathcal{R}=\{200,199,$\dots$,1\}$. To do so, if $\mathcal{O}$ is the observed order, we look at the quantity 
\begin{equation}
\Delta_{o}=\sum_{i=1}^{200}|\mathcal{R}_i-\mathcal{O}_i|.
\label{orderdistance}
\end{equation}
The smaller it is, the closer the 2 orders are. For each of the two experiments below, we ran the GSW 100 times and the deterministic GSW (DGSW) 100 times and recorded $\Delta_o$.

\subsubsection{Experiment 1}
We sampled 200 vectors $\textbf{v}_1,\dots,\textbf{v}_{200}$ in the ball of radius 1 of dimension 200. For each $\textbf{v}_i$, we replaced it by $i\cdot \textbf{v}_i/\|\textbf{v}_i\|$ so that $\|\textbf{v}_i\|=i$ for each vector. We call this vector set the incrementally growing norm set (IGN).


\subsubsection{Experiment 2}
We sampled 200 vectors $\textbf{v}_1,\dots,\textbf{v}_{200}$ in the ball of radius 1 of dimension 200. For each $\textbf{v}_i$, we replaced it by $X_i\cdot \textbf{v}_i/\|\textbf{v}_i\|$ so that $\|\textbf{v}_i\|=X_i$ for each vector, where $X_i=1$ if $i<n/2$ and 200 otherwise. We call this vector set the two group set (2G).

\subsubsection{Results}
We also ran the same experiments with 200 vectors of constant norm as a comparison, which we call the all equal norm set (AEN). The results are summarized in Figure \ref{norm_when_colored}.
\begin{center}
\begin{table}[h]
\begin{tabular}{l|lll}
& IGN & 2G & Control  \\
\hline
GSW & 18664.4 & 19997.32 & 13607.06 \\
DGSW & 18641.6 & 19996.16 & 13431.68
\end{tabular}
\caption{Result of our experiments on the moment of coloring depending on the norm of the vectors. The numbers shown are the $\Delta_o$ as defined in equation \ref{orderdistance}.}
\label{norm_when_colored}
\end{table}
\end{center}
We can see that the vector orders are actually further away from $\mathcal{R}$ than the random orders produced with constant vectors, which means that bigger vectors actually get colored later in the process. Additionally, the DGSW doesn't seem to yield significantly different results. While this is not what we expected, this behavior actually makes sense, because if we multiply $\textbf{v}_i$ by $\mu%\textbf{v}_i
$, it's corresponding coordinate in $\textbf{u}_t$ is going to be divided by $\mu$, thus it will move less towards the border of the hypercube and thus be colored later than the shorter vectors.

This motivates us to try to find a variant of the algorithm that would color bigger vectors earlier and thus perform better on an input such as $\{\textbf{v},\textbf{v},\textbf{v},3\textbf{v}\}$ for some $\textbf{v}\in\mathbb{R}^d$ and an arbbitrary $d\in\mathbb{N}$. One way would be to choose the pivot through some smart condition or to choose another feasible $\textbf{u}_t$ than the default least square solution with minimal norm, as in that specific adversarial example there are an infinite number of valid $\textbf{u}_t$, some of which solve our problem in one step with a perfectly balanced groups.

One such idea is to force the pivot to be the largest norm vector, and then, when $\textbf{v}_\perp=\textbf{0}$, to select $\textbf{u}_t$ through lasso with a very small alpha ($\alpha=10^{-32}$ for example) in order to ensure that the smallest number possible of coordinates are nonzero, and finally to select $\delta_t$ by taking it to be of the same sign as the coordinate of $x$ corresponding to the pivot (or randomly if that coordinate is 0). This variant solves the issue mentioned in the previous paragraph but loses a lot of randomness in the process, so there probably exist some different additional constraint to add when computing $\textbf{u}_t$ in colinear cases that could work even better.

Another variant that works but only in this trivial example and not in slightly more complicated examples with for examples 2 groups of vectors is to just force the largest alive vector to be the pivot at every step. This is just a product of the exact solution of the least squares we're choosing as there are infinitely many that wouldn't work. This doesn't work for slightly more convoluted adversarial cases and is just a product of our exact implementation.

A third variant that might help is to do quadratic programming instead of simple least squares when $\textbf{v}_\perp=\textbf{0}$. This way, we can force the quadratic program to minimize both $\|\textbf{Bu}_t\|$ but also $\|\textbf{u}_t\|$. This could be achieved by adding a line of 1's to the matrix $B$ containing all input vectors as column, and adding as conditions that the chosen $\textbf{u}_t$ must have a 1 in the pivot coordinate and 0s in already colored coordinates, similarly to the conditions in the original GSW system of equation. Then if the $\textbf{u}_t$ chosen through this quadratic program doesn't yield $\textbf{Bu}_t=\textbf{0}$, we discard it and compute $\textbf{u}$ through the usual method. This technique coupled with choosing the longest alive vector as pivot actually solves adversarial cases similar to those mentioned three paragraphs ago, as well as more complicated ones such as $\{\textbf{v},\textbf{v},\textbf{v},3\textbf{v},\textbf{w},\textbf{w},\textbf{2w}\}$, and doesn't modify the behavior of the algorithm when $\textbf{v}^\perp(t)\not=\textbf{0}$. The only small issue is that the matrix $\textbf{B}^T\textbf{B}$ then needs to be regularized by adding some very small constant times the identity or it isn't strictly positive definite.

%\subsection{What affects when a vector is colored other than the norm ?}

\subsection{Do longer vectors stay pivot for longer ?}\label{longer_vec_pivot_longer}
As longer vectors are colored later in the algorithm on average, one could think that they're staying as the pivot for longer. To test this hypothesis we design the following experiment.
\subsubsection{Experiment}
We sample 200 vectors of norm 1 in dimension 200 and multiply 100 of them by 200 (G1 with norm 1, G2 with norm 200). We then run the GSW 100 times with all these vectors as input and record for how long vectors of different norms (1 and 200) stay pivot once they become pivot. We call this measure the \textbf{ALOSAP} (average length of stay at pivot), and it is measured in number of while loop step.
\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll}
 &ALOSAP G1&ALOSAP G2\\
\hline
GSW&6.101&2.281\\
DGSW&5.989&2.277
\end{tabular}
\caption{Result of our experiments on whether long and short vectors stay for longer as the pivot. Results are averaged across groups from 100 runs.}
\label{pivot_longer}
\end{table}
\end{center}
Results are visible in Table \ref{pivot_longer}. We see that shorter vectors tend to stay pivot for much longer than their longer counterparts. This could be explained by the fact that, as seen in Section\ref{norm_affect_when}, coordinates of the update direction for all long vectors are very small so longer vectors rarely get colored by a small vector pivot, but shorter vectors do get colored while the longer vectors are pivot.

\subsection{Can we force bigger vectors to be colored earlier ?}
Another technique that we could use would be to modify the choice of the direction. Currently, the bigger a vector is the later in the process it will be colored. 

One could multiply the computed direction in each of its coordinate by the norm of the vector corresponding to that coordinate, or the squared norm. That would be a way of trying to compensate for the update coordinates of bigger vectors being smaller. This would remove the orthogonality of updates, but it doesn't mean that the assignment doesn't balance the vectors still.

We also study how that affects the order of the coloring in experiments similar to those done in subsection \ref{norm_affect_when}.
\subsection{Experiment 1}
We sample 100 points from the ball of radius 1 in dimension $d=2$ and run the classical GSW, as well as the two variant described in the current section 100 times each. We plot all vectors of imbalances obtained this way to see how these modifications affected the algorithm.
\subsubsection{Experiment 2}
We sample vectors similarly to the experiments performed in section \ref{norm_affect_when} and measure similarly how close the coloring order is to the order $\mathcal{R}=\{200,199,$\dots$,1\}$.
\subsection{Results}
\begin{figure}[h]
\include{3types_basis_100_0}
\caption{Plot of vectors of imbalances of group assignments from the GSW, the GSW with update direction multiplied coordinatewise by the norm of the corresponding vector, and the GSW with update direction multiplied coordinatewise by the square of the norm of the corresponding vector}\label{3_types_d_and_i}
\end{figure}

\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 &IGN w/ $D$&IGN w/ $D^2$& 2G w/ $D$&2G w/ $D^2$& AEN w/ $D$&AEN w/ $D^2$   \\
\hline
GSW&13246.72&6078.28&13246.74&6697.1&13444.94&13208.1\\
DGSW&6808.96&13100.02&13597.86&6830.82&13303.18&13344.14
\end{tabular}
\caption{Result of our experiments on trying to fix the later coloring of bigger norm vectors. The numbers shown are the $\Delta_o$ as defined in equation \ref{orderdistance}.}
\label{norm_earlier}
\end{table}
\end{center}
Results from Experiment 1 can be seen in Figure \ref{3_types_d_and_i}. The modification of the computation of the update direction didn't seem to completely ruin how far the sum of outputs were from \textbf{0} if we compare it with Figure \ref{4types_4}. Still, the vectors of imbalances  from modified GSW runs are more spread out than those from classical runs, which is because we forced our modifications onto the algorithm without letting it adapt.

We can see that our modifications indeed remove the late coloring. Multiplying once makes it so the vectors are colored approximately randomly and the multiplying twice makes it so the bigger vectors are colored earlier, as intended. 

There are very likely other ways of getting a similar effect that could increase the norm of the vector of imbalances so much, potentially by adding coordinates smartly.

\subsection{Can we find another way of computing the update direction ?}
 As we saw that larger vectors get colored later in the process on average when using the classical algorithm, one could ask themselves how to revert this effect. Let $\textbf{B}$ be the matrix containing our input vectors as columns. Using a singular value decomposition, we have that $\textbf{B}=U\Sigma V^T$ where $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ are orthonormal and $\Sigma\in\mathbb{R}^{m\times n}$ is all zeros except for the diagonal elements which are positive singular values. Using this decomposition, we can see that $\textbf{B}^+=V\Sigma^+U^T$ where $\Sigma^+$ is $\Sigma^T$ except nonzero entries $\sigma_i$ are replaced by their inverse $1/\sigma_i$. But if one replaced $\Sigma^+$ by $\Sigma$, then the  matrix multiplying the pivot vector would be $\textbf{B}^T$ instead of $\textbf{B}^+$. This suggestion from Pr. Marcus turned out not to work if we want to balance the vectors, but it actually does something close to the opposite which is very interesting. 

\subsubsection{Experiment 1}\label{exp1_A_T}

We sampled 200 vectors in dimension 200 in the ball of radius 1. We then ran the modified GSW algorithm where the next direction is computed via $\textbf{u}_t(\mathcal{A}_t\setminus\{p(t)\})=\textbf{B}_t^T\textbf{v}_{p(t)}$ and $\textbf{u}_t(p(t))=1, \textbf{u}_t(i)=0 \forall i\not\in\mathcal{A}_t$, where $\textbf{B}_t$ is the matrix containing the vectors that are still alive and not the pivot as columns. Everything else is kept similar. 

\subsubsection{Experiment 2}\label{exp2_A_T}


We run a similar experiment as in \ref{how_good_at_minimizing_disc} except we look for the discrepancy maximizing assignment via bruteforcing (\textbf{HDA}) and look at the naive walk trying to maximize the output norm (\textbf{MNW}).

\subsubsection{Results}
\begin{figure}[h]
\centering
\include{A_T_instead_of_lstsq_200}
\caption{Results of Experiment 1 (Section \ref{exp1_A_T}). Output sums using the modified GSW and DGSW where the update direction is computed by multiplying the pivot vector by $A^T$.}
\label{A_T_instead_of_lstsq}
\end{figure}
The outputs are shown in the figure \ref{A_T_instead_of_lstsq}. We can see that this modification seems like it now minimizes output balance instead of maximizing it, which was surprising to me at least. This seems like it could be useful to sample from unbalanced group assignments, or to find a subset to remove to maximize something. This might be equivalent to an already known algorithm, but if not I think there are probably interesting applications of this.

\begin{figure}[h]
\centering
\include{comparative_norms_n=160_repeat=1000_max_dim=32768.0_maximizing}
\caption{Results of Experiment 2 (\ref{exp2_A_T}): comparing the modified GSW and DGSW with discrepancy maximizing algorithms.}
\label{A_T_instead_of_lstsq_2}
\end{figure}
Results of experiment \ref{exp2_A_T} are available in Figure \ref{A_T_instead_of_lstsq_2}. We can see that for small $n$, using $\textbf{B}^T$ doesn't actually maximize the norm for small dimensions even though it seems to asymptotically do so when the dimension grows. For bigger $n$'s though, this variant seems to be decent at maximizing the norm of the vector of imbalance. It is also interesting to notice that there seems to be a change of regime around the point $n=d$, where after that point the average norm of the vector of imbalances goes up with the dimension whereas it seems to go down before.

\subsection{Are vectors with smaller dimensionality colored at the same moment as vectors with more dimensions ?}\label{smaller_dimensionality}
We want to know if vectors with a lot of 0's can be found among vectors that are less sparse, as that could be very interesting to solve various problems such as the planted clique. We will investigate how early they're colored on average.

\subsubsection{Experiment}
We sample 100 vectors of dimension 100 from the ball radius 1 but with 100 additional coordinates locked to zero. We also sample 100 vectors of dimension 200 from the ball of radius 1. We're interested in comparing whether vectors in one group get colored earlier than vectors from the other group on average. To do so we do 100 runs with each of three variants. In the first variant (V1), the vectors aren't changed. In the second one (V2), every vector is normalized. In the third one (V3), every vector is normalized but the non-sparse vectors are normalized to a norm of 2 so that the scale of the elements are similar to the sparse vectors normalized to a norm of 1. The last three variants are respective copies of the first three except the coordinates of the sparse vectors are shuffled so that the 0's aren's uniformly placed in the sparse group.

\subsubsection{Results}
%Table
\begin{center}
\begin{table}[h]
\begin{tabular}{l|llllll}
 &V1&V2&V3&V4&V5&V6\\
\hline
GSW&75.781&75.281&60.726&98.163&100.268&68.584\\
DGSW&75.393&76.594&61.692&97.231&98.883&68.088
\end{tabular}
\caption{Result of our experiments on whether sparse vectors are colored earlier. The numbers shown are the average coloring step of sparse vectors over 100 runs of the GSW.}
\label{pivot_longer}
\end{table}
\end{center}
We can see that sparse vectors are colored much earlier in the first 3 variants, and even earlier in the third variant, which might be explained also by their smaller norm relative to the non-sparse vectors. The last three variant show use that the earliness effect seems to be nearly completely linked to the fact that the sparse vector were not shuffled in the first three variants, as the sparse vectors are colored very close to the average of 100.

%Additional ideas
It could also be interesting to investigate how balanced each vector group is and how noise affect this result, as this could help us discover a hidden group in a larger set. Another interesting thing would be to see how varying the relative size of the two groups affects the phenomenon. Lastly, 
%We could also just measure this for sparse vectors that do not necessarily all have 0's on the same dimensions. DONE

\subsection{How often is the pivot vector colored ?}\label{pivot_colored}
It would be interesting to know how often the pivot is colored, and whether it depends on the pivot choice rule, or just on the vector set. To do so we try a couple different choice rules and vector set and investigate.
\subsubsection{Experiment}
We use three pivot choice rules: the \textbf{random} (R) rule, that is the classic one where the pivot is cosen uniformly at random when a new pivot is needed, the \textbf{maximum norm} one (MN) where the pivot is always chosen as the vector that has the greatest norm among all vectors that are alive, and the \textbf{norm-proportional} (NP) rule that makes it so each vector that is alive has a probability of being picked as the next pivot that is proportional to its norm. 

We use three different sets of 100 vectors in dimension 2,10 and 100, which correspond to those used in section \ref{norm_affect_when},that is the all equal norm set (AEN), the two group set (2G) where half the vectors have size 1 and the other half size $n=100$, and the incrementally growing norm set (IGN) where the vectors have respectively norms 1,2,...,100. We observe the proportion of time steps during which the pivot is colored among all time steps and try to see whether the pivot rule seems to make that proportion vary or whether it only depends on the vector set.

\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|lll|lll|lll}
Dimension &\multicolumn{9}{c}{\textbf{2}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.958&0.949&0.939&0.920&0.474&0.505&0.946&0.912&0.892\\
DGSW  &0.976&0.969&0.952&0.944&0.504&0.509&0.971&0.940&0.914\\
\hline
\hline
Dimension &\multicolumn{9}{c}{\textbf{10}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.822&0.798&0.822&0.657&0.357&0.346&0.761&0.578&0.628 \\
DGSW  &0.870&0.841&0.862&0.705&0.383&0.374&0.811&0.638&0.673\\
\hline
\hline
Dimension &\multicolumn{9}{c}{\textbf{100}}\\
Set  & \multicolumn{3}{c}{AEN} & \multicolumn{3}{c}{2G} & \multicolumn{3}{c}{IGN} \\
Rule &R&MN&NP&R&MN&NP&R&MN&NP\\ \hline
GSW  &0.532&0.527&0.537&0.434&0.438&0.420&0.463&0.362&0.421 \\
DGSW  &0.521&0.538&0.525&0.424&0.445&0.431&0.461&0.387&0.441\\
\end{tabular}
\caption{Result of our experiments on how often is the pivot colored as a function of the pivot choice rule and the input vector set. Results are averaged over 100 runs of the GSW each and are proportions between 0 and 1.}
\label{pivot_colored_results}
\end{table}
\end{center}
Results are available in Table \ref{pivot_colored_results}. Notice that when using the maximum norm mode with DGSW we lose all randomness, which is probably undesirable but these were still included for completeness.

The DGSW proportion all seem a little higher in the dimension 2 and 10 cases, while in dimension 100 there seems to be no significant differences. So coloring the closest point on the update direction seems to push toward coloring the pivot most of the time in smaller dimensions, but when the dimension grows this effect seems to vanish

As the vectors only have 2 potential different norms that are very different in size, we would expect the 2G set to not have much difference across the maximum norm and norm-proportional pivot choice rule, and that is indeed the case.

We can also see that the pivot rule does not seem to change the pivot coloring rate significantly in the AEN case.

We can see on the 2G and IGN vector sets that the proportion of colored pivots is lower when using non-uniformly random pivot rules which favor longer vectors, which is consistent with the observation that longer vectors get colored later from \ref{norm_affect_when}. This is absent on the AEN vector set as the norms are all equal.

Finally, the main observation seems to be that the pivot is colored more often when the dimension is small, which I currently don't have a logical explanation for.

\subsection{What if we choose the pivot as a function of the fractional coloring ?}\label{pivot_coloring_rules}
One could think of seeing the GSW as a rounding algorithm of some sort, and then, as the pivot is often colored as seen in Section \ref{pivot_colored}, we could want that the vectors closest to being colored are pivot as they're closer to being rounded. That is what we try in this section.% The pivot choice rule is also an interesting place to seek to optimize as it doesn't play a role inthe analysis from \cite{blues} and only plas a very minor role in section 6.2 of \cite{harshaw2019balancing}. Thus it leaves a lot of freedom that we can try to exploit to improve the GSW.

We introduce two new pivot choice rules: the \textbf{maximum absolute coloring} (MAC) rule that chooses as pivot the alive vector that is closest to -1 or 1 and separates ties uniformly randomly, and the \textbf{coloring proportional} (CP) rule that chooses the pivot according to a distribution such that the probability that an alive vector is chosen as the pivot is proportional to the absolute value of its current fractional coloring value, and choses uniformly randomly if the fractional coloring value of every alive vector is 0.

\subsubsection{Experiment 1}\label{exp_plot_max_col}
We sample 100 vectors of dimension 2 and run both the classical GSW with random pivot and the classical GSW with \textbf{maximum absolute coloring} (MAC) similar to Section \ref{pivot_colored_coloring_rules}, as well as the DGSW with the MAC pivot rule which makes it a deterministic algorithm given the first pivot. We do 100 runs per variant and plot the results to see whether there is some pattern and how it looks like, similarly to figure \ref{4types_3}.

\subsubsection{Results 1}
\begin{figure}[h]
\centering
\include{max_col_comparison}
\caption{Results of Experiment 1 (Section \ref{exp_plot_max_col}). Plot of vectors of imbalances for the GSW and DGSW with random pivot rule and MAC pivot rule. 100 points for each variant on the graph.}
\label{results_plot_max_col}
\end{figure}
Results are visible in Figure \ref{results_plot_max_col}. We can see that as previously, the vector of imbalance from DGSW run are slightly closer to 0 than those from GSW runs. Additionally, we can see that the MAC pivot rule seems to give us shorter vector of imbalances as well, with MAC DGSW runs being visibly closer to \textbf{0}. While this effect could be expected as we color the vectors that allow us to pick smaller deltas and thus move less away from \textbf{0}, it is actually very visible here and we will investigate it further in the next experiment.

\subsubsection{Experiment 2}\label{exp_norms_max_col}
We run a similar experiment as in Section \ref{how_good_at_minimizing_disc}, except the algorithms we use are the classical GSW, the GSW with MAC and the DGSW with MAC.
\subsubsection{Results 2}
\begin{figure}[h]
\centering
\include{comparative_norms_n=160_repeat=1000_max_dim=32768.0_max_col}
\caption{Results of Experiment 2 (Section \ref{exp_norms_max_col}). Average norm of 1000 vector of imbalances for $n=5,10,20,40,80,160$ input vectors and in dimensions $d=2$ to $d=2^{15}$, using GSW with both the random pivot rule and the maximal coloring rule, and DGSW with the maximal coloring rule.}
\label{results_plot_max_col_norms}
\end{figure}
We can see the results in Figure \ref{results_plot_max_col_norms}. The GSW and the GSW with MAC are very close, but the DGSW with MAC produces significantly smaller vector of imbalances. While for big dimension, it doesn't make much of a difference, for smaller dimensions and bigger numbers of vectors, this seems to be divide by more than 2 in comparison to the classical GSW which is a sizable improvement.

\subsubsection{Experiment 3}\label{pivot_colored_coloring_rules}
We reproduce the experiment in Section \ref{pivot_colored} but with the MAC and CP rules.

We would expect these methods to have higher pivot-coloring rates as the previous ones, as they're literally choosing elements that are more likely than average to be colored in the next step as their absolute value is closer to 1.
\subsubsection{Results 3}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll|ll|ll}
Dimension &\multicolumn{6}{c}{\textbf{2}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.950&0.945&0.590&0.550&0.939&0.900\\
DGSW  &0.965&0.968&0.600&0.566&0.954&0.927\\
\hline
\hline
Dimension &\multicolumn{6}{c}{\textbf{10}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.889&0.840&0.406&0.386&0.758&0.711\\
DGSW  &0.930&0.887&0.444&0.419&0.798&0.769\\
\hline
\hline
Dimension &\multicolumn{6}{c}{\textbf{100}}\\
Set  & \multicolumn{2}{c}{AEN} & \multicolumn{2}{c}{2G} & \multicolumn{2}{c}{IGN} \\
Rule &MAC&CP&MAC&CP&MAC&CP\\ \hline
GSW  &0.834&0.595&0.697&0.532&0.800&0.535\\
DGSW  &0.884&0.623&0.722&0.542&0.825&0.544\\
\end{tabular}
\caption{Result of our experiments on how often is the pivot colored as a function of the pivot choice rule and the input vector set, now with coloring-dependent pivot choice rules. Results are averaged over 100 runs of the GSW each and are proportions between 0 and 1.}
\label{pivot_colored_coloring_rules_results}
\end{table}
\end{center}
Results are visible in Table \ref{pivot_colored_coloring_rules_results}. We can see that as predicted, the pivot coloring rates are higher in this experiment than in section \ref{pivot_colored}, but they actually aren't for the 2 groups (2G) vector set using the random pivot. One potential explanation is that the longer vectors are often close to being colored but still harder to color than the small ones despite that, where the random pivot rule actually chooses a smaller vector 50\% of the time. 

For dimension 100 we see that the maximum absolute coloring rule actually yields a much higher pivot coloring rate than its proportional variant, which could be an indication that the fractional coloring become less sure, that is closer to 0 on average, the higher the dimension goes.

We can also notice that the DGSW leads to a higher coloring rate, which makes sense as we choose a pivot that's close to being colored and the DGSW will thus finish coloring it more often that the classical GSW.

\subsection{Can we choose the pivot from the potential update directions or $\textbf{v}^\perp(t)$ ?}
Section \ref{fast_computations} lets us compute the update direction and $\textbf{v}^\perp(t)$ for every potential pivot, so we could exploit that and choose among potential update directions, potentially only when we need a new pivot but not necessarily. Can we choose a direction such that its corresponding pivot gets colored at every time step ?

Definitions and experiments to be hopefully added for that subsection and others.
%\subsection{Can we force the algorithm to color the pivot vector ?}
%\subsection{What makes it so a vector is colored early outside of its norm and dimensionality ?}
%\subsection{Can we make the algorithm faster with some clever factorization ?}

\section{Finding structured subgroups}
We will try several experiments aiming to identify structured subgroups hidden in sets of random vectors. The idea is that the GSW should color members of the subgroup early as they should be easier to balance, and we will look at when the GSW colors which vectors over a large number of runs of the GSW and with various sets of vector and configurations. The ideal case would be if the GSW could help us solve a problem such as the Planted Clique.

This section is inspired by results such as those in section \ref{smaller_dimensionality}.

One problem that we would like to solve is, given $n-k$ vectors in a ball of radius 1 in some dimension $d$ and $k$ vectors in the same ball such that all their coordinates are positive, we apply a random rotation to the $k$ vectors. We would then like to use the GSW to find these vectors, by hoping that they would be colored early. We call this problem the \textbf{hidden cluster}.

One way to do so would be to look for the vector that's colored the earliest on average after some number$r$ of runs, and return that vector and the $k-1$ vector that are the closest to it. There are lots of moving parameters here, including $n,k,d$ and the specific GSW variant used. We will investigate when that approach works and when it doesn't. We call this technique the \textbf{early coloring}.

\subsection{How many runs are needed to get a decent idea of when a vector is colored ?}
In order to investigate that, we created some instances of the hidden cluster with $n=200,k\in\{5,10\},d=200$. We then used our early coloring method several times with $r=200$ and saw if the results differed over 5 runs for each set. As the end results were the same for each time we ran the method independently of which run it was, we concluded that $r=200$ was enough to get a good approximation of the average coloring times. 

This does not mean that the average coloring time of each vector were very close for each method run, but it means that the earliest vector was always the same.

\subsection{Does the early coloring technique work ?}
We want to know if the technique makes sense and works, so we created 50 instances of the hidden cluster with $n=200,k\in\{5,10,15\},d=200$, (so 100 for each $k$) and tried our technique with GSW, DGSW, MAC GSW and MAC DGSW, where MAC is defined in section \ref{pivot_colored_coloring_rules}. We then used our early coloring method with $r=200$ and recorded how often the technique successfully identified the hidden cluster. For DGSW with MAC which is actualy fully deterministic given the first pivot assuming random vector input, we made sure to use each of the 200 vectors as starting pivot exactly once.
\subsubsection{Results}
\begin{center}
\begin{table}[h]
\begin{tabular}{l|ll|ll|ll}
$k$  & \multicolumn{2}{c}{5} & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{15}\\
Rule &R&MAC&R&MAC&R&MAC\\ \hline
GSW  &0.23&0.29&0.39&0.46&0.71&0.63\\
DGSW &0.28&0.22&0.5&0.52&0.69&0.7\\
\end{tabular}
\caption{Result of our experiments on the early coloring method to solve the hidden cluster problem. The results are the proportion of instances for which the early coloring method worked out of 100 runs for each of the 12 cases.}
\label{early_coloring_method_exp1}
\end{table}
\end{center}
Results are available in Table \ref{early_coloring_method_exp1}. We see that the bigger the hidden cluster, the easier it seems to be to find which is expected as the random chance of finding it is also higher. Nevertheless for each dimension the proportion of runs in which the method works is way higher than the proportion of points that are part of the hidden cluster, so the method seems effective. Using the MAC pivot rule seems to help, but that is hard to tell for certain. On the other hand, the DGSW variant produces results that very slighlty better the original GSW variant, but this could very well be because of the small sample size.

%\subsection{Can we stop each run earlier ?}
%As we're only interested in which element is colored the earliest on average, we could try stopping early for example between the 50th and 100th time step in order to gain time in the process.

%\subsection{Can we use even less runs for our average ?}

\section{Next interesting directions}
%using the algorithm in machine learning to balance training and testing set, maybe balance batches in deep learning but probably too computationally expensive.
\newpage
%\appendix
%\section{This is an appendix}


\newpage
\nocite{*}
\bibliography{references}
\bibliographystyle{plain}

% Add the References to the table of contents.
\addcontentsline{toc}{section}{References}

\end{document}
